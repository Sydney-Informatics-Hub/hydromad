\documentclass[1p,authoryear]{elsarticle}

\usepackage{Sweave}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{amsmath}
%% link style
\usepackage{hyperref,color}
\definecolor{Red}{rgb}{0.5,0,0}
\definecolor{Blue}{rgb}{0,0,0.5}
  \hypersetup{%
    hyperindex = {true},
    colorlinks = {true},
    linktocpage = {true},
    plainpages = {false},
    linkcolor = {Blue},
    citecolor = {Blue},
    urlcolor = {Red},
    pdfstartview = {FitH},
    pdfpagemode = {UseOutlines},
    pdfview = {XYZ null null null}
  }

%% custom markup
\makeatletter
\newcommand\code{\bgroup\@makeother\_\@makeother\~\@makeother\$\@codex}
\def\@codex#1{{\normalfont\ttfamily\hyphenchar\font=-1 #1}\egroup}
\makeatother
\let\proglang=\textsf
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}
\def\ihacres{\textsc{ihacres}}
\def\Ihacres{\textsc{Ihacres}}
\def\RRMT{\textsc{rrmt}}
\def\R{\proglang{R}}

\journal{Environmental Modelling and Software}

%\VignetteIndexEntry{An open software environment for hydrological model assessment and development}
%\VignettePackage{hydromad}

\begin{document}

\begin{frontmatter}

\title{An open software environment for hydrological model assessment
  and development}

\author[icam,maths]{F. T. Andrews\corref{cor1}}
\ead{felix@nfrac.org}
\author[icam,ncgrt,maths]{B. F. W. Croke}
\author[icam,ncgrt]{A. J. Jakeman}

\cortext[cor1]{Corresponding author}

\address[icam]{Integrated Catchment Assessment and Management (iCAM) Centre,
Fenner School of Environment and Society, The Australian National University}

\address[ncgrt]{National Centre for Groundwater Research and Training}

\address[maths]{Department of Mathematics, The Australian National University}

\begin{abstract}
  The \pkg{hydromad} (Hydrological Model Assessment and Development)
  package provides a set of functions which work together to
  construct, manipulate, analyse and compare hydrological models. The
  class of hydrological models considered are dynamic (typically at a
  daily time step), spatially-aggregated conceptual or statistical
  models.  The package functions are designed to fit seamlessly into
  the \R{} system, and builds on its powerful data manipulation and
  analysis capabilities.  The
  framework used in the package encourages a separation of model
  components based on Unit Hydrograph theory; many published models
  are consistent with this and implementations of several are
  included.  For comparative assessment, model performance can be
  analysed over time and with respect to covariates to reveal
  systematic biases.  Support has been built in for event-based
  analysis of data and assessment of model performance. Fit statistics
  can be defined by choices of (1) temporal scale and aggregation
  function; (2) weighting and transformation; and (3) reference model.
  One can define new Soil Moisture Accounting models, routing models,
  calibration methods, objective functions, and evaluation statistics,
  while retaining as much of the default framework as is useful. And
  as the package code is available under a free software licence, one
  always has the freedom to adapt it as required.  Use of the software
  is demonstrated in a case study of the Queanbeyan River catchment in
  South-East Australia.
\\

\framebox{\parbox{0.9\textwidth}{\small
NOTICE: This is the author's version of a work accepted for publication
by Elsevier. Changes resulting from the publishing process, including
peer review, editing, corrections, structural formatting and other quality
control mechanisms, may not be reflected in this document. Changes
may have been made to this work since it was submitted for publication.
A definitive version was subsequently published in 
\emph{Environmental Modelling and Software} 26 (2011), 
\code{doi:10.1016/j.envsoft.2011.04.006}. 
}}

\end{abstract}

\begin{keyword}
Model evaluation \sep Hydrological models \sep Modelling frameworks
\sep Unit hydrograph \sep Event separation \sep R

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

\SweaveOpts{engine=R,eps=FALSE,echo=FALSE,prefix.string=figs/hydromad}

<<preliminaries, echo=FALSE, results=hide>>=
if (!file.exists("figs")) dir.create("figs")
library(hydromad)
library(xtable)
source("xtable_printbold.R")
library("mgcv")
ltheme <- col.whitebg()
sline <- ltheme$superpose.line
sline$lwd <- rep(1, 7)
sline$lty[2] <- 5
sline$lty[3] <- 6
sline$lty[4] <- 1
sline$lwd[4] <- 1.5
ltheme$superpose.line <- sline
ltheme$strip.background$col <- grey(7/8)
ltheme$strip.shingle$col <- grey(6/8)
ltheme$fontsize = list(text = 10)
lattice.options(default.theme = ltheme) ## set as default
ps.options(pointsize=10)
pdf.options(pointsize=10)
options(width=60, continue=" ")
set.seed(0)
## probability scale for normal variates
## TODO: use this by default in hydromad method? or move to latticeExtra
xscale.components.qnorm <- function(lim, ..., n = 5) {
    ans <- xscale.components.default(lim, ...)
    #p <- c(0.25, 0.1, 0.05, 0.01, 0.001, 0.0001)
    p <- c(0.1, 0.01, 0.001)
    p <- c(rev(p), 0.5, 1-p)
    at <- qnorm(p)
    ans$bottom$ticks$at <- at
    ans$bottom$labels$at <- at
    ans$bottom$labels$labels <- p * 100
    ans
}
@

<<define-periods>>=
data(Queanbeyan)

ts70s <- window(Queanbeyan, start = "1970-01-01", end = "1979-12-31")
ts80s <- window(Queanbeyan, start = "1980-01-01", end = "1989-12-31")
ts90s <- window(Queanbeyan, start = "1990-01-01", end = "1999-12-31")

ts73 <- window(Queanbeyan, start = "1973-01-01", end = "1976-01-01")
ts93 <- window(Queanbeyan, start = "1993-01-01", end = "1996-01-01")
ts83 <- window(Queanbeyan, start = "1983-01-01", end = "1986-01-01")
@


<<define-events>>=
evp <- 
    eventseq(Queanbeyan$P, thresh = 5, inthresh = 1,
         indur = 4, continue = TRUE)
ev83 <- window(evp, start = start(ts83), end = end(ts83))

Q <- ts83$Q
q90 <- quantile(coredata(Q), 0.9, na.rm = TRUE)
evq <- 
    eventseq(Q, thresh = q90, indur = 4,
             mindur = 5, mingap = 5, continue = TRUE)
evq.all <- 
    eventseq(Q, thresh = q90, indur = 4,
             mindur = 5, mingap = 5, all = TRUE)
@ 


\maketitle

\section{Introduction and motivation}

Catchment hydrology is the study of the water cycle at the scale of a
drainage basin, focusing for its practical importance on flow at the
catchment outlet. Central to this is the use of models to represent
processes at the catchment scale and evaluate the implications of
hypotheses of different model structures.

In different contexts, the level of detail may vary from simple
statistical or conceptual models to complex spatially-distributed or
physics-based models \citep{WheaterEtAl:1993}.  In practice all
catchment hydrology models need to be calibrated to measured data;
model parameters do not have a precise physical analogue when applied
at large scales \citep{WagenerEtAl:2009}.  The
simplest models of catchment hydrology dynamics are spatially lumped,
and conceptual or empirical in their approach. Such models can be used
to address questions in terms of aggregate effects, without
considering the detail of the processes involved. Often a single
dominant mode or process will be identified \citep{Young:2003}.  We
will focus on this class of models.

Models, although they are usually given names, should not be set in
stone. They encode sets of assumptions, which may be more or less
valid at different times, places, and scales; and, importantly, for
different purposes. Accordingly, models should be tested and evaluated
in the unique context of each application \citep{FeniciaEtAl:2008}.

Given the need for parsimonious models to address a range of
management and research problems, many have advocated flexible,
iterative model development processes \citep{FeniciaEtAl:2008,
  WagenerEtAl:2001}.  The top-down modelling
\citep{SivapalanEtAl:2003, LittlewoodEtAl:2003} and Data-Based
Mechanistic modelling \citep{YoungRatto:2008} agendas are particularly
prominent, and in fact these are quite similar to each other
\citep{Young:2003}.  Such processes involve intensive data analysis to
drive model development, with detailed comparison and evaluation of
model performance. Effective software support for such tasks is
crucial. There is a consequent need for flexible software environments
for hydrological modelling, tightly linked with data analysis and
model analysis methods.

A major challenge for modelling frameworks is to be flexible enough to
support creative problem solving in hydrology; As
\citet{Savenije:2009} argues, there is an art to hydrology that is
often not recognised.  However, flexibility ultimately needs to be
constrained by rigour.  Many authors argue for standardised tests and
comparisons of models \citep[e.g.][]{JakemanEtAl:2006, Beven:2008,
  DawsonEtAl:2007, DawsonEtAl:2010}.

A number of software frameworks for hydrological modelling have been
developed and are in active use, such as \textsc{oms}
\citep{LeavesleyEtAl:2006}, \textsc{prevah} \citep{ViviroliEtAl:2009},
\textsc{fuse} \citep{ClarkEtAl:2008} and \RRMT{} \citep{WagenerEtAl:2001}. 
Most such frameworks are designed for 
complex problem types, and necessarily restrict the analysis options
during model development. An exception is \RRMT{}, the Rainfall Runoff
Modelling Toolbox, which is used within the \proglang{MATLAB}
environment, and is therefore able to leverage the powerful data
manipulation functions it provides. Indeed, the software described in
this paper has been influenced by the design of \RRMT{}. However,
\proglang{MATLAB} has more of an engineering focus than a statistical
focus, and has rather different capabilities compared to primarily
statistical software. Furthermore these products are
closed-source, which obscures potentially important methodological
details, and withholds from users the freedom to adapt the code and
share their innovations.

This paper will introduce a software package for top-down modelling of
catchment hydrology, \pkg{hydromad}. It is based loosely on the unit
hydrograph theory of rainfall-runoff modelling, as described in Section
\ref{sec:model-framework}. \pkg{hydromad} is an open-source software
package for the \R{} system which is introduced in Section
\ref{sec:R}. As such it can be used cohesively with workflows based on
this increasingly popular software. Section \ref{sec:the-package}
covers the scope of the \pkg{hydromad} package and the functions it
provides. Two areas of focus for the package, and this paper, are
discrete event separation and the design of fit statistics,
discussed in Sections \ref{sec:events} and
\ref{sec:objective-functions} respectively.  Section
\ref{sec:data-checking} demonstrates how event-based data analysis can
be useful in a modelling context. In Section \ref{sec:calibration} we
demonstrate simple conceptual modelling; integral to this is a
detailed assessment of model performance, with a view to further model
development by discovering systematic biases, in
Section \ref{sec:assessment}. 

The example we will look at is the Queanbeyan River at Tinderry
streamflow gauge, near Canberra in South-Eastern Australia. It has a
catchment area of 490 square kilometers and is part of the Upper
Murrumbidgee catchment. It has seen much reduced river flow levels in
recent years. This catchment is unusual in that it
displays marked non-stationary response characteristics, and extended
drying periods with intermittent baseflow, making it
difficult to model \citep{KimEtAl:2007}.
Daily streamflow volume records are available from
\Sexpr{start(Queanbeyan)}, and the data used here extend to
\Sexpr{end(Queanbeyan)}.  Corresponding estimates of areal rainfall
were derived by spatial interpolation from several rain gauges
operated by the Australian Bureau of Meteorology and EcoWise. Daily
maximum temperature records from Canberra Airport were also used.


\section{The hydrological model framework}
\label{sec:model-framework}

The class of hydrological models considered here are dynamic,
spatially-aggregated conceptual or statistical models.  They
estimate streamflow at a catchment outlet, given inputs of areal
rainfall and potential evaporation (or, more commonly, temperature
data as an indicator of this), and potentially other inputs.  These
inputs and outputs are time series, typically at a daily time step,
and extending for many months or years.

As \emph{spatially lumped} models, they do not explicitly represent
spatial variation over the catchment area. In particular, the standard
formulations do not attempt to model effects of changes in land
cover. These models are usually calibrated to a period of observed
streamflow, and the parameters defining the modelled relationship
between rainfall, evaporation and flow are assumed to be
\emph{stationary} in this period.

The model framework used in the \pkg{hydromad} package is very
general, but encourages a separation of model components based on Unit
Hydrograph theory. This implies a two-component structure of a
\emph{soil moisture accounting} (SMA) module and a \emph{routing}
or \emph{unit hydrograph} module (Figure \ref{fig:model-framework}).
The SMA module converts rainfall and temperature into \emph{effective
  rainfall}: the amount of rainfall which eventually reaches the
catchment outlet as streamflow (i.e. that which is not lost as
evapotranspiration etc). The routing module converts effective
rainfall into streamflow, which usually amounts to convolving it with
a constant recession curve, the unit hydrograph. This structure is
consistent with \RRMT{} \citep{WagenerEtAl:2001}.

In fact, it is not strictly required to decompose a model this way: a
full model could be defined for the SMA component, with the routing
component omitted. It is worth noting, also, that the two components
are not necessarily simple models but may be composite models, and the
whole model may also be arranged into a composite structure.

\begin{figure}[hbpt!]
\setkeys{Gin}{width=0.9\textwidth}
\begin{center}
<<model-framework, fig=TRUE, width=6, height=1>>=
library(grid)
## framework diagram structure:
## --> |box| --> |box| -->
grid.newpage()
pushViewport(viewport(gp = gpar(fontsize = 10)))
arr <- arrow(length = unit(1, "char"))
## first arrows: rainfall and evaporation (inputs)
grid.lines(y = 0.75, x = c(0.0, 0.2), arrow = arr)
grid.lines(y = 0.5, x = c(0.0, 0.2), arrow = arr)
grid.lines(y = 0.25, x = c(0.0, 0.2), arrow = arr, gp = gpar(lty=2))
grid.text(y = 0.75, x = 0.1, label = "rainfall \n")
grid.text(y = 0.5, x = 0.1, label = "temp. / PET \n")
grid.text(y = 0.25, x = 0.1, label = "other inputs \n")
## first box: loss module
grid.rect(x = 0.3, width = 0.2, height = 0.9)
grid.text(x = 0.3, label = "Soil Moisture\nAccounting\n(SMA) model")
## second arrow: effective rainfall
grid.lines(y = 0.5, x = c(0.4, 0.6), arrow = arr)
grid.text(y = 0.5, x = 0.5, label = "effective\n rainfall")
## second box: routing module
grid.rect(x = 0.7, width = 0.2, height = 0.9)
grid.text(x = 0.7, label = "(unit hydrograph)\n routing model")
## third arrow: streamflow (output)
grid.lines(y = 0.5, x = c(0.8, 1.0), arrow = arr)
grid.text(y = 0.5, x = 0.9, label = "streamflow \n")
upViewport()
@
\caption{\label{fig:model-framework}
  The \pkg{hydromad} model framework, based on unit hydrograph theory.
}
\end{center}
\end{figure}

\setkeys{Gin}{width=1.0\textwidth}

A notable feature of the two-component model structure is that it
permits identification of the routing model in a way that is somewhat
de-coupled from identification of the soil moisture accounting model.
There are a number of different strategies that can be used to
calibrate a full hydrological model. The typical approach is a joint
optimisation of all parameters. Alternatively the unit hydrograph
could be estimated directly from streamflow data (once only, after
which it can be fixed), using inverse filtering
\citep{AndrewsEtAl:2010}, or average event unit hydrograph estimation
\citep{Croke:2006}. Or a simple data-based method could be used in the
SMA component to estimate effective rainfall, and this used for a
preliminary calibration of the routing model. 

A large number of published models are consistent with the unit
hydrograph framework. Several of these have already been implemented
in the \pkg{hydromad} package and are listed in Section
\ref{sec:the-package}.


\section{R: the synergies of community}
\label{sec:R}

\R{} is a language and environment for statistical computing and
graphics \citep{RCore, IhakaGentleman:1996}. It is based on the
high-level \proglang{S} language \citep*{BeckerEtAl:1988}, designed for
working with data and models. As it aims for maximum power and
flexibility, the primary mode of use is typing commands, or writing
scripts, rather than pointing and clicking.  As free (open source)
software, it has become a melting pot for computational statistics, as
evidenced by the number of contributed packages available, which has
grown exponentially since records began in 2001 \citep{Fox:2008}. This
opens up powerful synergies within and between research
communities.\footnote{The use of open source code also promotes
  academic integrity through ``reproducible research''
  \citep{GentlemanTempleLang:2007}.} 
As \citet{Bates:2008} puts it, ``I do not think of \R{} as a statistical
package or a product. To me, \R{} is a community.''

The \R{} community has developed world-class implementations of, for
example, generalised additive models \citep{Wood:2004}, 
time-varying linear models \citep{PetrisEtAl:2009}, 
forecasting methods \citep{forecast},
optimisation algorithms \citep{DEoptim}, 
data mining algorithms \citep{Williams:2009}, and 
statistical graphics \citep{Sarkar:2008, Wickham:2009}. 

The \R{} system thus provides a rich software ecosystem in which a
hydrological modelling framework can grow and evolve.


\section{The hydromad package}
\label{sec:the-package}

\subsection{Core functions}

The \pkg{hydromad} package provides a set of functions which work
together to construct, manipulate, analyse and compare hydrological
models. It is intended for:
\begin{itemize}
\item defining spatially-lumped hydrological
  models and fitting them to observed data; 
\item simulating outputs of these models, including any state
  variables;
\item evaluating and comparing these models: summarising
  performance by different measures and over time, using 
  graphical displays and statistics;
\item straightforward integration with other types of data
  analysis and model analysis in \R{}, including larger
  composite modelling studies in which rainfall runoff is just
  one part.  
\end{itemize}

The main design goals of the package were \emph{flexibility}, to
explore research questions and develop improved methods, and
\emph{simplicity} of the basic framework. In terms of flexibility, one
can define new Soil Moisture Accounting models, new routing models,
new calibration methods, new objective functions, and new evaluation
statistics, while retaining as much of the default framework as is
useful. And as the package code is available under an open source
licence, one always has the freedom to adapt it as required.

Particularly strong support has been built in for event-based analysis
of data and model performance. This involves isolating relatively
discrete events from time series, and analysing statistical properties
of the events, rather than the traditional approach of using every
time step at which data were recorded.

The package functions are designed to fit seamlessly into the \R{}
system. Consistent with other modelling functions in \R{}, many of the
functions in \pkg{hydromad} are implemented as methods of standard
generic functions. A constructor function creates a \code{hydromad}
object, and this can be passed on to methods for calibration,
analysis, reporting, etc. The object encapsulates a model
(composed of a SMA model and/or a routing model), with specified
parameter values or parameter ranges, along with the data and model
outputs.

\begin{table}[ht]
  \begin{center}
    \begin{tabular}{ll}
      \hline
      \code{hydromad()} & specifies a model, with fixed and/or free parameters \\
      \hline
      \code{update()} & modifies the structure, parameters or data of an existing model \\ 
      \hline
      \code{fitBy...()} & calibrates a model to data (several methods) \\
      \hline
      \code{predict()} & simulates streamflow (etc.) from a fitted model \\
      \hline
      \code{simulate()} & generates a set of models by sampling over parameter ranges \\
      \hline
      \code{summary()} & calculates fit statistics and other information \\
      \hline
      \code{runlist()} & constructs a named list of models for comparative analysis \\
      \hline
    \end{tabular}
    \caption{
      Core modelling functions provided by the \pkg{hydromad}
      package. See text for more.
    } 
    \label{tab:functions}
  \end{center}
\end{table}

The core modelling functions are listed in Table \ref{tab:functions}.
Of course, each of these functions has several arguments. The
details are given in help pages, which can be accessed within \R{}, or
online at \url{http://hydromad.catchment.org/}. A tutorial document is
also available.

Other standard \R{} methods are provided for convenience to work with
\code{hydromad} objects: for example one can extract parameter values
with \code{coef()}, and model results with \code{fitted()},
\code{observed()}, or \code{residuals()}. An estimate of the parameter
variance-covariance matrix can be extracted with \code{vcov()} where
applicable.  Several 
plotting functions are also available; \code{xyplot()} is the basic
function for displaying time series and scatter plots, while
\code{qqmath()} shows empirical cumulative distributions (often called
``flow duration curves'' in hydrology). Most methods will
work either on a single model object or on a \code{runlist}.

\subsection{The models}

Currently, the package currently includes implementations of several
published SMA models: 
\begin{itemize}
\item \ihacres{} Catchment Wetness Index (CWI) model
  \citep{JakemanHornberger:1993} including the generalisation of
  \citet{YeEtAl:1997}, modified according to \citet{CrokeEtAl:2005}. A
  temperature-dependent drying rate is 
  used to estimate a wetness index, which defines the runoff ratio.
  This model includes a scale factor which is
  estimated by mass balance with observed streamflow (or based on the
  gain of the transfer function used for routing, if applicable). 
\item \ihacres{} Catchment Moisture Deficit (CMD) model
  \citep{CrokeJakeman:2004}. Accounts for evapo-transpiration and
  changes in catchment storage. 
  The version use here includes a power law form, and is described in
  Section \ref{sec:cmd-model};
\item Sacramento Soil Moisture Accounting model
  \citep{Burnash:1995} developed by the US National Weather
  Service. With 13 parameters it is more complex than the other models
  listed here. Many published studies have used this model, often with
  good results. This implementation uses code from the University of
  Arizona. 
\item the GR4J model \citep{PerrinEtAl:2003}, mod\`ele du G\'enie
  Rural \`a 4 param\`etres Journalier. In fact this is split up into a
  1-parameter SMA model and a 3-parameter routing model. The
  non-linear routing component is based on a groundwater reservoir.
\item the AWBM, Australian Water Balance Model
  \citep{Boughton:2004}. In its simplest form this consists of a
  1-parameter SMA model, although the full form has 6 parameters. It
  is traditionally used with a two-store (3 parameter) routing
  component. 
\item the single-bucket models of \citet{BaiEtAl:2009}, including
  interception, saturation excess runoff and subsurface flow.
\item a degree-day factor snowmelt model of \citet{KokkonenEtAl:2006}.
  A fraction of rainfall becomes snow, based on temperature
  thresholds, and a snow reservoir is estimated. Discharge from the
  snow model is, currently, fed into the CMD model listed above.
\end{itemize}

A set of simple benchmark models is also available; these are useful
for some kinds of calibration methods, and for null models in a
comparative analysis. Note that the \code{dbm} and \code{runoffratio}
models make use of observed streamflow and so can not be used for
general simulation. The benchmark models are:
\begin{itemize}
\item \code{scalar}: a constant runoff ratio (i.e. effective rainfall
  is a constant fraction of rainfall). The fraction is estimated for
  mass balance with streamflow, or based on the gain of the transfer
  function used for routing, if applicable.
\item \code{intensity}: runoff ratio estimated by raising rainfall to
  a power, up to threshold rainfall rate with maximum runoff. With a
  power of 0 this reduces to the scalar model.
\item\code{runoffratio}: a runoff ratio, estimated by a moving average
  through the data, is used to scale rainfall.
\item \code{dbm}: observed streamflow raised to a power defines an
  index of antecedent wetness. This index, possibly lagged,
  is used to scale the rainfall. As a typical structure used in
  the early stages of Data-Based Mechanistic modelling
  \citep{Young:2003}, it is termed \code{dbm} in \pkg{hydromad}.
\end{itemize}

Routing models currently include 
\begin{itemize}
\item \code{armax}: ARMAX-type (auto-regressive, moving average, with
  exogenous inputs), also known as linear transfer functions
  \citep{JakemanEtAl:1990, Ljung:1999};
\item \code{expuh}: exponential component configurations (up to 3 in
  parallel and/or series). The time constants of each are specified,
  as well as a choice of configuration. A loss term can be included to
  represent simple groundwater exchange, similar to the form of
  \citet{HerronCroke:2009}.
\item \code{powuh}: a power law form of the unit hydrograph,
  parameterised according to \citet{Croke:2006}.
\item \code{varuh}: a variable partitioning extension of a 2-store
  model: this is an example of a routing model which is not a constant
  unit hydrograph, but rather depends on the level of rainfall.
\end{itemize}

Note that \code{armax} models up to third order can be converted into
\code{expuh} form within \pkg{hydromad}. For these models, efficient
estimation methods are available: notably the Simple Refined
Instrumental Variable (SRIV) algorithm \citep{Young:2008}, and an
inverse filtering algorithm which estimates the parameters directly
from streamflow data.  In the case of \code{expuh}, if the solution
does not make sense physically --- having negative or imaginary
recession rates --- then these are re-fitted with constraints.

\subsection{Optimisation algorithms}

Several optimisation functions are available in the \pkg{hydromad}
package for calibrating models to observed data. The different
algorithms may each be preferred for different types of problems, and
most have settings to tune their performance. There is generally a
trade-off between rapid convergence to a moderately good result, versus
a time-consuming search for the best possible solution. The choice
in this regard will depend on the task at hand. 

\begin{figure}[hpbt]
\begin{center}
<<optimtrace-plot, fig=TRUE, width=7, height=3>>=
load("hydromad_optimtraces.Rdata")
xyplot(traces, superpose = TRUE, ylim = c(0.60, NA), xlab = "Function evaluations",
       ylab = "objective function value",
       auto.key = list(corner = c(1, 0.02))) +
    glayer(i <- length(na.trim(y, sides = "right")), 
           panel.text(x[i], y[i], group.value, cex = 0.75,
                      pos = if (group.value == "DREAM") 3 else 4))

print(trellis.last.object())
@ 
\caption{\label{fig:optimtrace} 
  Optimisation traces from 7 algorithms in calibrating an
  8-parameter model (described in the text) using an $R^2$ objective function.
  Dream was run using a likelihood function analogue to the objective function.
  Note that each algorithm was run using default settings only, and
  some of the results could probably be improved by adjusting these settings. 
}
\end{center}
\end{figure}

The performance of optimisation algorithms, defined further on, can be
visualised with an optimisation trace plot, showing the improvement in
the solution found with increasing effort. Effort in this sense means
the number of times the model simulation function is run, which is
generally proportional to the total running time. One example is given
in Figure \ref{fig:optimtrace}, which shows results from calibrating
the \ihacres{} CMD model with a three-store unit hydrograph, involving
3 free parameters in the SMA component and a further 5 free parameters
in the routing component.  It was calibrated to a three-year period of
streamflow in the Queanbeyan River starting in 1983. The objective
function used was the $R^2$ using untransformed data
\citep{NashSutcliffe:1970}.  Note that these results are only for
illustration of the general point, and that the algorithms were run
only once with the somewhat arbitary default settings.  In this case
the \code{PORT} algorithm found a good result relatively quickly, but
in other cases the evolutionary algorithms such as \code{DE},
\code{SCE} or \code{Dream} may find better solutions.  It is often the
case that if a fit is needed quickly, such as in a large simulation
study or an interactive, exploratory analysis, then algorithms such as
\code{PORT} or \code{Nelder-Mead} are most appropriate.

The current set of general optimisation functions is:
\begin{itemize}
  \item \code{fitBySampling}: allows for Random, Latin Hypercube or
    regular gridded sampling. The model with best objective function
    from the sample is selected. Note that these sampling methods can be used for more
    general analysis with the \code{simulate()} function; an example
    is given in Section \ref{sec:calibration}.
  \item \code{fitByOptim}: uses \R{}'s built-in \code{optim}
    and \code{nlminb} functions to optimise an objective function. The initial
    parameter values are chosen from a preliminary sampling run, or
    alternatively, a number of samples can be used as different
    starting points (multi-start mode). 
    Some of the available methods are:
    \begin{itemize}
      \item \code{"PORT"}, using functions from the Bell Labs \textsc{PORT}
        library\footnote{http://www.bell-labs.com/project/PORT/};
      \item \code{"Nelder-Mead"}, a simplex method;
      \item \code{"BFGS"}, a quasi-Newton method;
      \item \code{"SANN"}, Simulated Annealing, designed to find a
        reasonable global solution even in ill-conditioned,
        high-dimensional solution spaces.
    \end{itemize} 
  \item \code{fitBySCE}: the Shuffled Complex Evolution algorithm
    developed at the University of Arizona \citep{DuanEtAl:1992}; 
  \item \code{fitByDE} Differential Evolution \citep{PriceEtAl:2006},
    provided by the \pkg{DEoptim} package; and 
  \item \code{fitByDream} DiffeRential Evolution Adaptive Metropolis
    \citep{VrugtEtAl:2009}. This is a Markov-Chain Monte Carlo method,
    giving probabilistic results. However, it can also be used simply as a
    optimisation algorithm. 
\end{itemize}

In addition, some specialised calibration functions are available for
specific models. 
It is straightforward, too, to make use of any other general
optimisation function to calibrate \code{hydromad} model objects.

The important issues of event-based analysis and design of objective
functions are discussed in the following two sections.


\section{Discrete event separation}
\label{sec:events}

We are often interested in hydrological response properties, and
modelling these, at the event scale rather than at the level of the
raw data. Furthermore, model residuals are typically highly
autocorrelated, which is problematic when attempting to assess model
performance. An attractive approach is to separate the streamflow
record into relatively isolated events, and work with attributes of
events rather than time steps.  Events are most often used in the
literature for extreme value analysis \citep{KatzEtAl:2002} or for
ephemeral flow systems in arid environments
\citep{McIntyreAlQurashi:2009}, but can also be applied more
generally. For instance \citet{Willems:2009} uses event windows to
extract local peaks and troughs of a streamflow series.
\citet{BoyleEtAl:2000} separate streamflow series into events, but do
model assessment using the raw time-step data rather than event-scale
properties.

One might be concerned that, in reducing a series with thousands of
observations to perhaps a hundred events, we are throwing away most of
the information; but we argue that the latter is often a more
reasonable representation of the actual information content of the
data for the purposes at hand.

The \pkg{hydromad} package provides functions for identifying events
based on thresholds and various timing criteria, and applying
aggregation functions to events. Events can be characterised by
attributes such as total flow or rainfall, antecedent conditions,
temperature or season. 

Temporal aggregation is more typically undertaken in terms of regular time
steps, such as days or months. This is certainly easier, but is
arguably less meaningful in hydrological terms, and is susceptible to
edge effects. Event-based aggregation requires consideration of how
events are to be defined for the purposes at hand. Also, as events may
have widely differing durations, in some cases it may be appropriate
to weight events by their duration for fitting or assessment.

Events may be defined either from a rainfall series or from
a streamflow series (or both). Rainfall-based events are more suitable for
assessing SMA model performance, because even when there is no
streamflow response to rainfall, this is an important feature of the
model. Streamflow-based events are more suitable for investigating
streamflow characteristics such as unit hydrograph estimation or flood
frequency analysis.

There are several other considerations too: are only the high periods
above a threshold of interest, or should such a high period instead
initiate an event window which continues until the next such event; or
should the high and low periods around a threshold both be considered?
Should events be terminated by flow (or rainfall) falling below a
threshold, or falling below a threshold for a certain time, or must
they be separated from other events by a minimum time? Should events
of too short a duration be skipped, or extended?
To help one decide these issues, the \pkg{hydromad} package provides a
graphical user interface for interactively testing different event
definitions on rainfall and streamflow time series.

Several event definitions are used in some of the pre-defined statistics
listed in \ref{appendix:statistics}, and throughout this paper. They
are also illustrated in Figure \ref{fig:ts83-events}.
\begin{itemize}
  \item \code{e.rain5}: events are defined by rainfall exceeding 5 mm
    per day, and continuing until the next such event. Each single event
    continues at least until rainfall remains below 1 mm for 4 days.
  \item \code{e.q90}: events are defined by observed flow exceeding
    the 90 percentile level for at least 2 time steps, and continuing
    until the next such event. Each single event continues at least
    until flow falls below the 90 percentile level for 4 time steps,
    and must be separated from the next event by a further 5 time steps.
  \item \code{e.q90.all}: events are defined by observed flow
    exceeding the 90 percentile level for at least 2 time steps, but
    unlike \code{e.q90}, are not continuing until the next event;
    rather, these high flow periods and the low flow periods between
    them are both considered as events. The high events continue until
    the flow falls below the 90 percentile level for 4 time steps and
    must be separated from each other by a further 5 time steps.
\end{itemize}


\begin{figure}[hpbt]
\begin{center}
<<ts83-events-plot, fig=TRUE, width=7, height=4.5>>=
xyplot(ts83[,c("P","Q")], strip = FALSE, xlab = NULL,
       ylab = "mm / day",  layout = c(1, NA), 
       scales = list(x = list(axs = "i")),
       lattice.options = list(skip.boundary.labels = 0),
       ylab.right = rev(c("(rainfall)", "e.rain5", "e.q90", "e.q90.all")),
       type = list("s","l"))[c(1,2,2,2)] +
    layer_(panel.xblocks(evp, col = c("grey95", "grey90"), border = "grey80"), rows = 1:2) +
    layer_(panel.xblocks(evq, col = c("grey95", "grey90"), border = "grey80"), rows = 3) +
    layer_(panel.xblocks(evq.all, col = c("grey95", "grey90"), border = "grey80"), rows = 4)
print(trellis.last.object())
@ 
\caption{\label{fig:ts83-events} 
  Section of the Queanbeyan River dataset, showing areal rainfall (P) and
  streamflow (Q). The event windows corresponding to the
  \code{e.rain5}, \code{e.q90} and \code{e.q90.all} statistics
  are shown. The alternating shading marks the event windows; there is no
  difference between the light and dark shaded events.
}
\end{center}
\end{figure}




Finally, the choice of aggregation function is crucial: it is typically the sum
or mean, but could also be a quantile or a set of quantiles. Figure
\ref{fig:evp5-acf-plot} shows that taking flow sums in the
\code{e.rain5} event windows does eliminate the auto-correlation evident
in the raw time series. 


\begin{figure}[hpbt]
\begin{center}
<<evp5-acf-plot, fig=TRUE, width=7, height=2.5>>=
Q <- ts83$Q
#ev <- eventseq(Q, thresh = quantile(coredata(Q), 0.9, na.rm = TRUE),
#               mingap = 5, mindur = 2, all = TRUE)
evsums <- eventapply(Q, evp, FUN = sum)
library(coda)
xyplot.list(list(raw = as.mcmc(coredata(na.omit(Q))),
                 events = as.mcmc(coredata(na.omit(evsums)))),
            FUN = acfplot, aspect = "fill", 
            ylim = c(-0.25,NA),
            xlab = c("Lag (days)", "Lag (events)"))
print(trellis.last.object())
@ 
\caption{\label{fig:evp5-acf-plot} 
  Demonstration of event aggregation of the streamflow data
  from Queanbeyan River, period 1983-01-01 to 1986-01-01.
  Events were defined from the rainfall series using the
  \code{e.rain5} definition. 
  Note, specifically, that the first-order autocorrelation evident in
  the raw data is reduced in the event series.
}
\end{center}
\end{figure}



\section{Statistics and Objective Functions}
\label{sec:objective-functions}

At least as important as the choice of calibration algorithm is the
design of an objective function. This requires careful consideration
of the intended purpose of the model, how it will be applied, what
derived values will be ultimately used, and the level of temporal
precision required. 

Streamflow data tends to be highly skewed, and this leads implicitly
to a large weighting put on a few large observations, a weighting
which is often inappropriate for the purposes of the model and the
uncertainty in those particular values.

\citet{Croke:2009} describes an approach to incorporating uncertainty of
individual data values into the formulation of an objective function.
This is useful where detailed information on data errors are
available.  Otherwise, weighting can be managed by a simple
transformation of the data: for instance, if we assume that errors in
the data are multiplicative, an inverse-variance weighting would
correspond to a log transformation. To reduce skewness further, a
Box-Cox transform \citep{BoxCox:1964} can be chosen such that the
observed data approximates a Normal distribution. This is demonstrated
in Figure \ref{fig:boxcox-plot}. 

\begin{figure}[hpbt]
\begin{center}
<<boxcox-plot, fig=TRUE, width=7, height=3>>=
Q <- ts70s$Q
library(car)
start <- quantile(coredata(Q[which(Q > 0)]), 0.1,
                  na.rm = TRUE, names = FALSE)
lambda <- coef(powerTransform(coredata(na.omit(Q)) + start))
qqmath(~ data | which, 
       make.groups(raw = coredata(Q),
                   log = log10(coredata(Q) + start),
                   box.cox = bcPower(coredata(Q) + start, lambda)),
       scales = list(y = list(relation = "free", draw = FALSE)),
       layout = c(NA,1),
       type = "l", 
       ylab = "data (arbitrary scale)",
       xlab = "cumulative probability % (Normal distribution)",
       xscale.components = xscale.components.qnorm,
       f.value = ppoints(100), tails.n = 100) +
    layer(panel.qqmathline(..., probs = c(0.1, 0.9), lwd = 2))
print(trellis.last.object())
@ 
\caption{\label{fig:boxcox-plot} 
  Demonstration of a Box-Cox transformation and log transform of
  the streamflow data from Queanbeyan River, period 1970-01-01 to 1980-01-01.
  A reference line is shown through the 10th and 90th percentiles on the
  normal probability scale, showing deviations from normality.
}
\end{center}
\end{figure}


In terms of the typical assessment of goodness of fit of a simulated
time series to the observed time series, there are three main facets
to the design of a statistic: 
\begin{enumerate}
  \item temporal precision / aggregation: either regular time steps
    may be specified, or hydrological events may be separated in some
    way. The choice of aggregation function is crucial: it is
    typically the sum or mean, but could also be a quantile or a set
    of quantiles.
  \item data transformation: to adjust the implicit weighting put on
    different flow levels, and the corresponding sensitivity of fit
    statistics. 
  \item reference model: a null model to use as a reference point to
    compare model performance
    \citep[after][]{NashSutcliffe:1970}. This is typically as simple
    as a constant at the mean observed level, but more informative choices
    are possible. The choice of reference model does not change the
    ranking of models, but does change the scale of the statistic.
\end{enumerate}


The \pkg{hydromad} package allows statistics and objective functions
to be specified as arbitrary \R{} functions. A helper function
is available to streamline the process of defining a statistic based
on the above considerations: \code{buildTsObjective()} takes optional
arguments for grouping, an aggregation function, a data transformation
in the Box-Cox family of transformations, and a reference model.

The statistics defined by \code{buildTsObjective()}, and in most of
the pre-defined statistics, are based on a general form of a fit
statistic, termed \code{nseStat()}. It is based on the
familiar $r^2$, a more general form of the Nash-Sutcliffe Efficiency
\citep{NashSutcliffe:1970}, defined as
\begin{equation}
  \mathrm{nseStat}(Q,X) = r^2 = 1 - \frac{ \sum |Q_* - X_*|^2 }{ \sum |Q_* - Z_*|^2 }
\end{equation}
where $Q$ and $X$ are the observed and modelled values respectively,
possibly in aggregated form; $Z$ is the result from a reference model,
which is the baseline for comparison. It defaults to the mean of
observed data after transformation
$\mathrm{E}(Q_*)$, corresponding to the typical $r^2$ statistic.
Subscript $*$ denotes an arbitrary transformation.

The set of pre-defined statistics is listed in
\ref{appendix:statistics}. 
These statistics can be used, combined and adapted, either as
objective functions or for model evaluation (discussed later).



\section{Data analysis methods}
\label{sec:data-checking}

As the models considered here are data-driven, there is a core role
for data analysis in model development and assessment.
Exploratory data analysis is inherently open-ended and should adapt to
the unique problems at hand. As a set of possible starting points for
analysis, these type of methods are often useful:

\begin{itemize}
  \item interactive inspection of time series data. The importance of
    ``eyeballing'' the data in detail is sometimes forgotten. 
  \item cross-correlation: reveals strength of the
    linear relationship between pairs of time series, and can be applied over time to
    identify non-stationarity of the relationship. 
  \item trend estimation by smoothing, where trend can be considered
    generally as any systematic relationship with covariates,
    including (but not limited to) time.
  \item where spatially distributed data is available, such as records from
    multiple rain gauges, it is often worth checking for spatial effects.
\end{itemize}

Rainfall patterns are complex, with dynamics from time scales of hours
up to decades, and, importantly, with semi-regular seasonal effects.
One can easily generate a seasonal and trend decomposition of the
rainfall series using the STL algorithm \citep{ClevelandEtAl:1990},
shown in Figure \ref{fig:stlplot}. This reveals a semi-regular
seasonal pattern and a long-term non-seasonal climatic pattern: the
period from 1992 onwards appears to be much less variable across
years.  Unexplained, short-term variation remains and the wet period
of 1991 for example appears as inconsistent with the general seasonal
pattern.

\begin{figure}[hpbt]
\begin{center}
<<stlplot, fig=TRUE, width=7, height=4.5>>=
x1m <- aggregate(Queanbeyan, as.yearmon, sum)
x3m <- simpleSmoothTs(x1m, width = 3)
stlfit <- stl(na.approx(sqrt(x3m$P)), s.window = 10, robust = TRUE)
foo <- 
    xyplot(stlfit,
           type = c("l", "g"), 
           strip.left = "strip.default",
           xlab = NULL, ylab = expression(sqrt("rainfall (mm/month)")),
           scales = list(x = list(tick.number = 30)))
pad <- lattice.getOption("axis.padding")$numeric
seriesH <- sapply(foo$panel.args, function(args) 
                  diff(extendrange(args$y, f = pad)))
print(resizePanels(foo, h = seriesH))
@
\caption{\label{fig:stlplot} 
  Seasonal and Trend decomposition by Loess (STL) applied to the
  monthly areal rainfall series (with a moving average over 3 months)
  for Tinderry catchment. 
  The Loess window for seasonal extraction was set to 10 years. 
}
\end{center}
\end{figure}


Of course, many hydrological problems are concerned with the
streamflow response to rainfall which occurs at shorter time scales. A
good first step in characterising this response is to examine the
auto-correlation and cross-correlation functions.  These represent
average responses. It is also possible to look at distributions of
responses in terms of discrete events.

\subsection{Regression analysis}

Rainfall-runoff dynamics were investigated in the Queanbeyan river
dataset. Events were defined by rainfall exceeding 5 mm per day,
and continuing until the next such event: the \code{e.rain5}
definition from Section \ref{sec:events}. 
To estimate the effective rainfall, rises in streamflow were
extracted, and scaled for mass balance over the whole period of
record. This simple approach assumes that the unit hydrograph is
constant. A more sophisticated analysis could use a time-varying
parameter model \citep[e.g.][]{NortonChanat:2005}.
Once effective rainfall is estimated, it is straightfoward
to calculate the runoff ratio corresponding to each
event, and relate this to other variables. The obvious drivers are
rainfall amount and antecedent wetness. Streamflow itself can be used
as an index of wetness, assuming a direct storage-discharge
relationship \citep{Young:2003}. Temperature or season may
also capture some residual effects related to dryness or rainfall
intensity.

Generalised Additive Models (GAMs) are suitable for this type of
analysis. Rather than assuming a functional form, it is possible to
allow the data to define the relationships: using splines as a basis,
the degree of smoothness is chosen by generalised
cross-validation. This is implemented in the \R{} package
\pkg{mgcv} \citep{Wood:2004}.

<<lag-time-by-events>>=
#armod <- arima(Queanbeyan$Q, order = c(2,0,0), include.mean = FALSE)
#QbynInnov <- zoo(pmax(residuals(armod), 0), time(Queanbeyan))
QbynInnov <- pmax(diff(Queanbeyan$Q, na.pad = TRUE), 0)
QbynInnov[QbynInnov < 0.01] <- 0
#QbynInnov <- zapsmall(QbynInnov, digits = 3)
QbynInnovU <- QbynInnov * (mean(Queanbeyan$Q, na.rm=TRUE) / 
                           mean(QbynInnov, na.rm=TRUE))

pEvents <- 
    cbind(peakP = eventapply(Queanbeyan$P, evp, FUN = max, na.rm = TRUE),
          peakQ = eventapply(QbynInnovU, evp, FUN = function(x) max(x,0,na.rm=TRUE)),
          anteQ = eventapply(lag(Queanbeyan$Q, -2), evp, FUN = head, 1),
          anteE = eventapply(lag(Queanbeyan$E, -2), evp, FUN = head, 1))
#pEvents$yday <- julian(time(pEvents)) %% 365.25

ok <- (pEvents$peakQ > 0)
pEvents <- pEvents[ok,]

delays <- 
    eventapply(merge(QbynInnov, Queanbeyan$P), evp,
               FUN = estimateDelay, by.column = FALSE,
               lag.max = 5, rises = FALSE, negative.ok=TRUE)
@ 


\begin{figure}[hpbt]
\begin{center}
<<event-runoff-gam-plot, fig=TRUE, width=7, height=3.5>>=
gamdat <- with(pEvents,
               data.frame(runoff = peakQ / peakP,
                          peakrain = peakP,
                          anteflow = pmax(anteQ, 1e-3),
                          temperature = anteE))
qbGam <- 
    gam(log2(runoff) ~ s(log2(anteflow)) + s(log2(peakrain)) + s(temperature), 
        data = gamdat)

par(mfrow = c(1,3), mar = c(5,2,0,0))
plot(qbGam, residuals = TRUE, shade = TRUE)
par("mfg" = c(1,1)); abline(h = 0, lty = 3)
par("mfg" = c(1,2)); abline(h = 0, lty = 3)
par("mfg" = c(1,3)); abline(h = 0, lty = 3)
@ 
\caption{\label{fig:event-runoff-gam-plot} 
  rainfall-runoff proportion, estimated in each
  event window, related to three potentially
  relevant variables (antecedent streamflow as an index of wetness,
  peak rainfall, and antecedent temperature) by fitting a GAM. 
  The panels show additive effect sizes in terms of deviations from
  the average runoff ratio, in log units. Standard errors (shaded) and
  partial residuals (points) are also shown.
}
\end{center}
\end{figure}

Figure \ref{fig:event-runoff-gam-plot} shows the estimated effects
of the three variables mentioned above. The model was formulated as
<<echo=TRUE, eval=FALSE>>=
gam(log2(runoff) ~ s(log2(anteflow)) + s(log2(peakrain)) + s(temperature))
@ 
where \code{s()} is the smoothing operator. This implicitly includes
an intercept term; therefore the estimated terms represent effects as
deviations from the average runoff ratio (on a log scale). 
The strongest relationship is with antecedent wetness, and this
appears to be a power law (linear on a log-log scale). It is also
obvious that rainfall intensity increases the runoff ratio for
rainfall rates above about 20 mm/day. An additional effect of
temperature can be seen, and is significant given the standard error
bounds shown.

This type of analysis can be used to inform conceptual model
development by revealing the form of the effects that need to be
accounted for. We will revisit these effects when assessing model
performance in Section \ref{sec:assessment}: by assessing the
effectiveness of a model in accounting for each effect we can see
which of the process representations needs to be improved. A more
sophisticated analysis might also estimate interactions between these
variables.

\subsection{Delay times}

Dynamic hydrological models are typically calibrated with an implicit
emphasis on streamflow peaks, and are therefore particularly sensitive
to timing mismatches between rainfall and streamflow. Most models can
only handle a constant delay time. However, it is reasonable to expect
that the delay between rainfall and runoff is variable (e.g. due to
rainfall intensity, and the timing of rainfall within each time
step). Timing mismatches may also in some cases be due to errors in 
the data.

In some problem contexts, such as estimation of sediment load, it is
important to account for high flows but not their timing.  In this
case all flow peaks could be shifted in line with corresponding
rainfall peaks, to match the typical model assumptions.

Using the event-based approach, we can analyse delay times directly.
In each event window, the delay time is estimated from the cross
correlation function of rainfall with streamflow rises.  Event windows
with no streamflow response (or a response of less than 0.1 mm/day)
were ignored.


<<lag-times, results=tex>>=
peakQ.ML <- pEvents$peakQ * 506
## leave out events with no delay estimate
peakQ.ML <- peakQ.ML[time(na.omit(delays)),]
## cut into groups with equal numbers of values
peakQ.breaks <- quantile(coredata(peakQ.ML), c(0, 0.333, 0.667, 1))
flowcut <- cut(peakQ.ML, breaks = peakQ.breaks, 
               labels = c("lowest third", "middle third", "highest third"))
lagtab <- 
    prop.table(table(flowcut, na.omit(delays)), 1)
lagtab <- round(addmargins(lagtab, 2) * 100, 1)
xtable(lagtab, digits = 0, 
       caption = sprintf(
       "Distribution of lag times following identified rainfall events (above 5 mm/day), 
conditioned on the magnitude of the resulting streamflow peak. Each range of magnitudes covers an 
equal number of events (%d), and values shown are percentages in each range. The lowest third of 
events have peaks below %d ML/day, and the highest third have peaks above %d ML/day.", 
       round(NROW(flowcut)/3), round(peakQ.breaks[1]), round(peakQ.breaks[2])),
       label="tab:lag-times")
@

The estimated delay times are shown in
Table \ref{tab:lag-times}. The table is divided according to the magnitude
of peak streamflow: it is clear that small runoff events have a longer
and more variable delay time (with a modal time of 2 days), while
large runoff events have a more consistent delay time of 1 day. This
makes sense as large runoff events would take a more direct flow
path. Whether it is worth trying to model this effect would depend on
the problem context. Also, some of the outlying delay times may be
worth investigating as possible errors. 

Having satisfied some basic checks, we can move on to
analyse the dynamics in more detail using conceptual models.


\section{Model development and calibration}
\label{sec:calibration}

This section introduces simple conceptual modelling of the catchment system.
A 3-year calibration period was chosen to focus the model development:
1983-01-01 to 1986-01-01.

\subsection{The CMD model}
\label{sec:cmd-model}

For a Soil Moisture Accounting model, we start with the
\ihacres{} Catchment Moisture Deficit (CMD) model of
\citep{CrokeJakeman:2004}. This accounts for evapo-transpiration and
changes in catchment storage in a mass balance equation:
\begin{equation}
M[t] = M[t-1] - P[t] + E_T[t] + U[t]
\end{equation}
where $M$ represents catchment moisture deficit (CMD) in mm, constrained
below by 0 (the nominal fully saturated level).  $P$ is catchment areal
rainfall, $E_T$ is evapo-transpiration, and $U$ is drainage
(effective rainfall), in units of mm per day.

Rainfall effectiveness (i.e. drainage proportion) is a simple
\emph{instantaneous} function of the CMD level, dropping to zero at 
$M = d$. In the default linear form of the model, the rainfall
effectiveness $\mathrm{d}U / \mathrm{d}P$ decreases linearly from
the full runoff level at $M = 0$. In the power law form it is:
\begin{equation} \label{eq:cmd-power}
\frac{\mathrm{d}U}{\mathrm{d}P} = 1 - \min\left[1, \left(\frac{M}{d}\right)^\eta\right]
\end{equation}
where $\eta$ is the \code{shape} parameter. The actual drainage each
time step involves the integral of Equation \ref{eq:cmd-power}:
\begin{equation}
  M_f = \left\{
    \begin{array}{ll}
      d \left(-(1-\eta) \frac{P_t - (M_{t-1}-d)}{d} + 1\right)^{\frac{1}{1-\eta}} & \text{if } M_{t-1} \leq d\\
      d \left(-(1-\eta) \frac{P_t}{d} + (\frac{M_{t-1}}{d})^{1-\eta} \right)^{\frac{1}{1-\eta}} & \text{if } d < M_{t-1} \leq d + P_t\\
      M_{t-1} - P_t & \text{if } M_{t-1} > d + P_t\\
    \end{array} \right.
\end{equation}
Similar solutions for the linear and trigonometric forms are given in
\citet{CrokeJakeman:2004}. 

Evapo-transpiration, as a proportion of the potential rate \code{E[t]},
is also a simple function of the CMD level, with a threshold at $M = fd$:
\begin{equation}
E_T[t] = e E[t] \min\left[1, \exp\left(2\left(1 - \frac{M_f}{fd}\right)\right)\right]
\end{equation}
where $M_f$ is the CMD level $M$ after precipitation and drainage have been
accounted for. 

Following \citet{CrokeJakeman:2004}, we fix the evapotranspiration
coefficient $e$ at a reasonable value for use with
daily maximum temperature data: 0.166. 
The flow threshold $d$ has been found not to be a very sensitive
parameter, and was fixed at a value of 200 mm. This leaves the stress
threshold $f$ and shape parameters to be calibrated. Reasonable ranges
were selected for these, shown in Table \ref{tab:cmd-parameters}.


\begin{table}[ht]
  \begin{center}
    \begin{tabular}{lp{10cm}}
      \hline
      \code{f} & stress threshold / wilting point (as fraction of
      \code{d}). Range $[0.01, 1]$ \\
      \hline
      \code{shape} & power in drainage equation. Value less than 1 select
      the linear form; a value of 1 selects the trigonometric form.
      Range $[0, 100]$ \\ 
      \hline
      \code{d} & flow threshold (mm). Fixed at 200. \\ 
      \hline
      \code{e} & evapotranspiration coefficient. Fixed at 0.166. \\
      \hline
    \end{tabular}
    \caption{
      Parameters of the CMD Soil Moisture Accounting model, and values
      used in this study.
    } 
    \label{tab:cmd-parameters}
  \end{center}
\end{table}


\subsection{Routing component}

For the routing component we use an exponential form of the unit
hydrograph with two stores in parallel.\footnote{This is using the
  \code{expuh} routing function, which currently handles up to three
  stores in parallel and/or series.}
This form has been found to perform well in a variety of studies
\citep[e.g][]{JakemanEtAl:1990}. 
Each component is defined by a time constant $\tau$ and fractional
volume $v$, or equivalently a recession rate $\alpha$ and peak response
$\beta$. They are related as $\alpha = \exp(-1 / \tau)$ and 
$\beta = v (1 - \alpha)$ for each component.
When there are two components in parallel, these are conventionally
called slow ($s$) and quick ($q$) flow components. The total simulated
flow $X$ at each time step $t$ is the sum of the two:
\begin{align} \label{eq:expuh2}
X_s[t] &= \alpha_s X_s[t-1] + \beta_s U[t] \notag \\
X_q[t] &= \alpha_q X_q[t-1] + \beta_q U[t] \notag \\
X[t] &= X_s[t] + X_q[t]
\end{align}

Only one of the two fractional volume parameters needs to be
specified; the other is the remainder, ensuring that the total volume
is unchanged. Therefore this routing component has 3 free parameters,
and the full model has 5 free parameters. 


\subsection{Simulation over parameter spaces}

The widespread observation of equifinality \citep{Beven:2006}, whereby
parameter values can not be uniquely identified on the basis of the
available data, should make us cautious when calibrating models. It
can be useful to visualise the \emph{objective function surface in
  parameter space} corresponding to the calibration problem. This is a
non-trivial problem and there are various possible approaches
\citep{WagenerKollat:2007}. Apart from the computational demands of
random sampling, there are significant cognitive demands in attempting
to visualise multi-dimensional functions.

Of particular interest is comparing different objective functions over
the parameter space. We generated 2000 stratified random samples over
pre-defined parameter ranges, and calculated fit statistics from each
simulation: \code{r.squared} and \code{r.sq.log}, defined in
\ref{appendix:statistics}. For each statistic, the best 3 simulations
were identified, and also what might be termed a \emph{90\% coverage
  set}, following \citet{BlasoneEtAl:2008}: the smallest set of the
best-performing parameter values, such that 90\% of the observed
streamflow levels fall within the range of the ensemble simulation.

\begin{figure}[hpbt]
\begin{center}
<<parameter-spaces, fig=TRUE, width=7, height=5.5>>=
load("hydromad_sims.Rdata")
simsets <- make.groups(r.squared = data.frame(gluer2$feasible.set),
                       r.sq.log = data.frame(gluer2log$feasible.set),
                       nonfeasible = sims[,1:5])

auto.key <- 
    list(text = c("r.squared", "r.sq.log"), lines = TRUE, padding.text = 2,
         corner = c(0.8, 0.1), background = "white", border = TRUE,
         title = "Fit statistic /\n objective function", cex.title = 1)

foo <- 
splom(simsets[,1:5], groups = simsets[["which"]],
      par.settings = simpleTheme(lwd = 2, pch = c(16:17)),
      auto.key = auto.key, xlab = NULL,
      axis.text.cex = 0.7, jitter.x = TRUE, jitter.y = TRUE,
      panel = function(...) NULL,
      lower.panel = function(...) NULL) +
    glayer(panel.xyplot(..., pch = ".", col.symbol = "grey"), groups = 3) +
    glayer(ii <- chull(x, y),
           panel.polygon(x[ii], y[ii], ..., col = NA, border = col.line),
           panel.xyplot(x[1:3], y[1:3], jitter.x = FALSE, jitter.y = FALSE, ...),
           groups = 1:2)
print(foo)
@ 
\caption{\label{fig:parameter-spaces}
  Optimum points and regions of parameter space, projected onto each
  pair of parameters. Symbols show the best 3 simulations according to
  each statistic, and lines show convex hulls around the corresponding
  \emph{90\% coverage sets}. Small points represent all 2000 simulations.
  These simulations used a CMD model with 2-store routing applied to
  the period 1983-01-01 to 1986-01-01 of the Queanbeyan River data.
}
\end{center}
\end{figure}

The \emph{90\% coverage sets} and optimum values are compared between
the two objective functions for each pair of parameters in Figure
\ref{fig:parameter-spaces}. The results show a reasonable level of
agreement between the two statistics, as one would hope to be the
case. The parameters \code{shape} and \code{tau_s} can take on values
over their full prior range, although they do constrain the parameter
space through their interactions (e.g. high \code{shape} and low
\code{tau_s} is unacceptable according to the \code{r.sq.log} statistic).
There are differences between the objective functions too:
\code{r.sq.log} statistic favours longer time constants and a higher
fractional volume of 
slow-flow ($v_s$). Regarding the SMA component, there appears to be an
interaction between the two parameters (f and shape), and an ambiguity
in where the optimum lies. While most of the optima are around
$\mathrm{shape} = 0, f = 0.7$, which is a linear form of the drainage
equation, one result favoured by \code{r.squared} is $\mathrm{shape} =
32, f = 0.8$, which is a power law form. In fact, with such a high
power the drainage equation is almost a step function. This ambiguity
is explored in the following sections.

Random sampling is extremely inefficient for exploring high
probability density regions of the parameter space. For models with
more parameters, a random sampling approach will not be able to find
optimum parameter sets in a reasonable amount of time. Adaptive
sampling schemes, such as Markov Chain Monte Carlo (MCMC) methods, have
been shown to be essential in such contexts
\citep[e.g.][]{BlasoneEtAl:2008}. As such, the \pkg{hydromad} package  
has a function to estimate feasible parameter sets from the output of
the DREAM MCMC algorithm, as well as from purely random sampling.


\subsection{Calibration}

In order to assess the model performance in detail it is useful to
identify two or three models which capture aspects of our uncertainty
about the parameter values and representation of processes.

The CMD model structure as described above was fitted to the
calibration period data using two contrasting
objective functions: the typical \code{r.squared} using raw data
\citep{NashSutcliffe:1970}; and the same with log transformed data
(\code{r.sq.log}). A log transform would be appropriate when assuming
multiplicative data errors, and naturally gives less weight to peak
flows than with raw data. As above, the observed 10 percentile flow
was used as an offset. In this case the \code{PORT} method of
\code{fitByOptim} was used for calibration. The two resulting
parameter sets capture the ambiguity in the SMA parameters $f$ and
shape noted in the previous section, as can be seen from the values
given in Table \ref{tab:fits83-coef}.

One more model was selected to act as a reference. The
\emph{intensity} SMA model estimates runoff from rainfall on the
corresponding time step only, neglecting any consideration of
antecedent conditions. In terms of the effects shown in Figure
\ref{fig:event-runoff-gam-plot}, only the peak rainfall effect in the
middle panel is included.  This provides a useful reference, because
any improvement over this can be largely attributed to the dynamic
process representation of the CMD model. The \emph{intensity} model
is defined as:
\begin{equation}
  \frac{U}{P} = c \min\left(1, \left(\frac{P}{P_\mathrm{max}}\right) ^ \gamma\right)
\end{equation}

<<>>=
load("hydromad_fits83.Rdata")
fits83 <- runlist(cmd_raw = fits83.raw$cmd,
                  cmd_log = fits83.log$cmd,
                  intensity = fits83.raw$intensity)
modlist <- update(fits83, newdata = Queanbeyan)
tsVerif <- Queanbeyan
tsVerif$Q[time(ts83)] <- NA
veriflist <- update(fits83, newdata = tsVerif)
@ 

This model, together with the same two-store routing component used
with the CMD model, were calibrated to the \code{r.squared} objective
function. Calibrated parameter values were 
$\gamma$ = 
\Sexpr{round(coef(fits83[["intensity"]])[["power"]], 2)}, 
$P_\mathrm{max}$ = 
\Sexpr{round(coef(fits83[["intensity"]])[["maxP"]], 0)}, 
and $c$ = 
\Sexpr{round(coef(fits83[["intensity"]])[["scale"]], 2)}.
The unit hydrograph routing parameters are given in Table
\ref{tab:fits83-coef}. 

\begin{table}[hbt]
\begin{center}
<<fits83-coef-table, results=tex>>=
tmp <- coef(fits83)[,c("f","shape","tau_s","tau_q","v_s","delay")]
print(xtable(tmp, digits = c(0,2,0,0,2,2,0)), floating = FALSE)
@
\caption{\label{tab:fits83-coef}
  Calibrated parameters for models calibrated to the period 1983-01-01 to
  1986-01-01 for the Queanbeyan River catchment. 
}
\end{center}
\end{table}

\begin{figure}[hpbt]
\begin{center}
<<modlist-plot, fig=TRUE, width=7, height=3>>=
xyplot(fits83[1:2], superpose = TRUE, grid = TRUE,
       auto.key = list(columns = 3),
       #xlim = as.Date(c("1995-01-01", "2005-01-01")), ## TODO: doesn't work!?
       #cut = list(n = 3, overlap = 0), 
       strip = FALSE,
       lattice.options = list(skip.boundary.labels = 0),
       ylim = c(10^-2, 20), xlab = NULL,
       yscale.components = yscale.components.fractions,
       xscale.components = xscale.components.subticks,
       scales = list(y = list(log = TRUE), 
                     x = list(axs = "i")))
print(trellis.last.object())
@ 
\caption{\label{fig:modlist-plot} 
  Fitted streamflow time series from \ihacres{} CMD models, compared
  to the observed time series.
}
\end{center}
\end{figure}

A sample of the simulated time series from the two calibrated CMD
models, compared to the observed streamflow time series, is shown in
Figure \ref{fig:modlist-plot}. As the plot is on a log scale, it
highlights the low flow dynamics; other aspects of the model fit are
considered in Section \ref{sec:assessment}. The most obvious feature
of the plot is that the \code{r.squared} calibration is 
over-estimating most of the low flows and lacking any response to the
smaller peaks. This reflects the skewness in the raw data and the
consequent overpowering influence of peak values on the objective
function.  Somewhat surprisingly, many of the large peaks in the
period shown are also over-estimated by the \code{r.squared} case,
though this is not generally the case, as we will see. Both models
fail to reproduce the switching off of baseflow in 2003 and 2004, as
there is no such mechanism in the model formulation. 

The next section will investigate the performance of the three
candidate models in detail, with a view to further model development.


\section{Model performance assessment}
\label{sec:assessment}

The typical approach to model performance assessment is to calculate
fit statistics. Fit statistics for the calibration period are given in
Table \ref{tab:fits83-calibration}. The same fit statistics were also
calculated over two verification periods --- the relatively wet decade
of the 1970s and the relatively dry decade of the 1990s --- and are
listed in Tables \ref{tab:fits83-verif70s} and
\ref{tab:fits83-verif90s}. The statistics are defined in
\ref{appendix:statistics}. 

\begin{table}[hpbt]
\begin{center}
  \subfloat[Calibration period 1983--1986]{
    \label{tab:fits83-calibration}
<<fits83-calibration-table, results=tex>>=
stats <- c("r.squared", "r.sq.log", "e.rain5", "e.rain5.log", "e.q90", "e.q90.all")
tmp <- summary(fits83, stats = stats)
printbold(xtable(tmp, digits = 2), floating = FALSE)
@
  } \\
  \subfloat[Wet period 1970--1979]{
    \label{tab:fits83-verif70s}
<<fits83-verif70s-table, results=tex>>=
tmp <- summary(update(fits83, newdata = ts70s), stats = stats)
printbold(xtable(tmp, digits = 2), floating = FALSE)
@
  } \\
  \subfloat[Dry period 1990--1999]{
    \label{tab:fits83-verif90s}
<<fits83-verif90s-table, results=tex>>=
tmp <- summary(update(fits83, newdata = ts90s), stats = stats)
printbold(xtable(tmp, digits = 2), floating = FALSE)
@
  }
\caption{\label{tab:fits83-fit-stats}
  Fit statistics for three candidate models calibrated to the Queanbeyan
  River data. The best-performing model according to each statistic is
  highlighted in each assessment of three periods.
}
\end{center}
\end{table}

The fit statistics show that, while the \code{r.squared} model does
best in terms of its own objective function, its improvement over the
\emph{intensity} reference model appears to be modest in the wet
period, but more pronounced in the dry period. In contrast, the \code{r.sq.log} model
appears to do much better than the others in terms of its objective
function, and also in terms of all the event-based statistics, even
those without a log transformation of the data.

\begin{figure}[hpbt]
\begin{center}
<<modlist-rsquared-ts, fig=TRUE, width=8, height=4.5>>=
rsq.ts <- do.call(cbind, lapply(modlist, summary, stats = "r.squared", breaks = "1 years", with.hydrostats = FALSE))
rsqlog.ts <- do.call(cbind, lapply(modlist, summary, stats = "r.sq.log", breaks = "1 years", with.hydrostats = FALSE))
hydrostats <- summary(modlist[[1]], stats = c(), breaks = "1 years")
foo <- 
c(r.squared = xyplot(rsq.ts, superpose = TRUE, type = "s", ylim = c(-0.1, NA), 
                     auto.key = list(columns = 3), xscale.components = xscale.components.subticks),
  r.sq.log = xyplot(rsqlog.ts, superpose = TRUE, type = "s", ylim = c(-0.1, NA)),
  runoff = xyplot(hydrostats$runoff, type = "s"),
  layout = c(1,NA)) +
    layer_(panel.xblocks(ts83$Q, col = "#eeeeee", border = "lightgrey"))
foo <- update(foo, scales = list(y = list(rot = 0)),
              xlab = NULL)
print(foo, panel.height = list(x = c(3, 3, 1), units = "null"))
@ 
\caption{\label{fig:modlist-rsquared-ts} 
  Fit statistics calculated in simulation over each calendar year 
  for three candidate models calibrated to the Queanbeyan River
  data. The calibration period is shaded.
}
\end{center}
\end{figure}

It is easier to see the pattern of the fit statistics when it is
displayed graphically, and with finer time resolution. Figure
\ref{fig:modlist-rsquared-ts} shows the \code{r.squared} and
\code{r.sq.log} statistics in each calendar year for the three
models. An interesting feature that can be seen is that the
\emph{cmd\_log} model (i.e. that calibrated to \code{r.sq.log})
actually performs better than the \emph{cmd\_raw} model in terms of
\code{r.squared} in many years: specifically in dry years such as
1993, 1994 and 1996. In wet years the \emph{cmd\_log} model does less
well, whereas the \emph{intensity} model is often one of the best
performers on this measure. The story is different on the measure of
\code{r.sq.log}, where the performance is dominated by the
corresponding \emph{cmd\_log} model.

\begin{figure}[hpbt]
\begin{center}
<<modlist-residuals-plot, fig=TRUE, width=8, height=4.5>>=
sresids.raw <- 
    simpleSmoothTs(na.aggregate(residuals(modlist)), 
                   width = 365, c = 2, n = 500)
sresids.log <- 
    simpleSmoothTs(na.aggregate(residuals(modlist, boxcox = 0)), ## i.e. log transform
                   width = 365, c = 2, n = 500)

foo <- 
c("smoothed raw residuals" =
  xyplot(sresids.raw, superpose = TRUE, grid = TRUE, abline = list(h = 0),
         auto.key = list(columns = 3),
         scales = list(x = list(axs = "i")), xlab = NULL,
         lattice.options = list(skip.boundary.labels = 0),
         xscale.components = xscale.components.subticks) + 
    layer_(grid.text(x = 0, y = 0, "(overestimated)", just = c(-0.1, -0.5), gp = gpar(col = "grey30")),
           grid.text(x = 0, y = 1, "(underestimated)", just = c(-0.1, 1.5), gp = gpar(col = "grey30"))),
  "smoothed log residuals" =
  xyplot(sresids.log, superpose = TRUE, grid = TRUE, abline = list(h = 0),
         auto.key = list(columns = 3),
         scales = list(x = list(axs = "i")), xlab = NULL,
         lattice.options = list(skip.boundary.labels = 0),
         xscale.components = xscale.components.subticks) + 
    layer_(grid.text(x = 0, y = 0, "(overestimated on log scale)", just = c(-0.1, -0.5), gp = gpar(col = "grey30")),
           grid.text(x = 0, y = 1, "(underestimated on log scale)", just = c(-0.1, 1.5), gp = gpar(col = "grey30"))),
  "observed streamflow" =
  xyplot(simpleSmoothTs(na.approx(observed(modlist[[1]])), width = 365, c = 2, n = 500),
           grid = TRUE, scales = list(x = list(axs = "i"))),
  layout = c(1, NA)) +
    layer_(panel.xblocks(ts83$Q, col = "#eeeeee", border = "lightgrey"))
print(foo, panel.height = list(x = c(3, 3, 1), units = "null"))
@ 
\caption{\label{fig:modlist-residuals-plot} Time series of model
  residuals, both raw and log-transformed (with an offset). The daily
  residuals are smoothed over an effective bandwidth of 1 year with a
  triangular kernel. This reveals longer-term biases while hiding any
  short-term errors. The calibration period is shaded.
}
\end{center}
\end{figure}

The fit statistics we have considered so far have been based on
comparing observed and simulated values at a daily time step. It often
of interest also to consider the model performance at a longer time
scale, to reveal the size and direction of any systematic
biases. Figure \ref{fig:modlist-residuals-plot} shows this in a plot
of model residuals smoothed over a time window of around 1 year. Both
raw and log-scale residuals are considered, by analogy with Figure
\ref{fig:modlist-rsquared-ts}. Indeed, the story is similar at this
scale: the \emph{cmd\_raw} model does best on the raw scale in wet
years, but does less well than \emph{cmd\_log} in dry years. All
models have drastically underestimated the exteme response in years
1974--1975.


%\begin{figure}[hpbt]
%\begin{center}
%<<fig-model-event-sums, fig=TRUE, width=7, height=3, eval=FALSE>>=
%xyplot.list(modlist, 
%            FUN = event.xyplot, events = evp,
%            extract = fitted, formula = ~ pmax(e(Q), 0.01), 
%            scales = list(log = TRUE), 
%            xscale.components = xscale.components.log10ticks,
%            yscale.components = yscale.components.log10ticks,
%            aspect = "iso", 
%            grid = TRUE, layout = c(3, 1), 
%            xlim = c(0.1, NA),
%            ylim = c(0.1, NA), 
%            se = FALSE, abline = list(a = 0, b = 1, lwd = 2),
%            ylab = "predicted flow volume (mm)", 
%            xlab = "observed flow volume (mm)",
%            x.same = TRUE)
%print(trellis.last.object())
%@
%\caption{\label{fig:model-event-sums} Modelled vs observed
%  total flow in discrete event windows over verification period.
%}
%\end{center}
%\end{figure}


This kind of model performance assessment is quite detailed, and is
able to show which models are preferred, over time and from different
perspectives. However, it is ultimately unsatisfying because it does
not provide much guidance in the task of diagnosing model structural
problems and developing an improved model. For this, we can invoke
an event-based analysis and consider the model residuals in
relation to specific features of the data.

\begin{figure}[hpbt]
\begin{center}
<<res-each-plot, fig=TRUE, width=8, height=4.5>>=
foo <- 
event.xyplot(modlist, events = evp,
             ylim = c(-30, 60),
             formula = ~ 
             log2(e(pmax(lag(Q,-2),0.01),first)+.01) +
             log2(e(P,max)+.01) +
             e(E,mean) +
             sqrt(e(simpleSmoothTs(lag(P,-1),width=30,c=1,sides=1),first)) +
             log2(e(U,max)+.01) +
             log2(e(Q,mean)+.01),
             auto.key = list(lines = TRUE, points = FALSE, columns = 3),
             se = FALSE, pch = NA) +
    glayer(panel.rug(x = x, alpha = 0.5), groups = 1) +
    layer(grid.text(x = 0, y = 0, "(overestimated)", just = c(-0.1, -2.0), gp = gpar(col = "grey30")),
          grid.text(x = 0, y = 1, "(underestimated)", just = c(-0.1, 1.5), gp = gpar(col = "grey30")),
          packets = 1)
dimnames(foo)[[1]] <- 
    c("log2 ante. flow", "log2 max rain", "mean temp. (deg. C)",
      "sqrt 30-day ante. rain", "log2 max eff. rain (U)", "log2 mean flow"
      )
plot(foo)
@ 
\caption{\label{fig:res-each-plot} 
  Marginal effects of variables on model residual flow sums in
  \code{rain5} event windows.
  These are GAM smooths fitted independently to the residuals
  against each covariate. All covariates are in units of mm/day
  (before transformation) except temperature which is in degrees Celcius.
}
\end{center}
\end{figure}


\begin{figure}[hpbt]
\begin{center}
<<resbc-each-plot, fig=TRUE, width=8, height=4.5>>=
foo <- 
event.xyplot(modlist, events = evp,
             extract = function(x) residuals(x, boxcox = 0), ## i.e. log transform
             ylab = "residual sums of log flow in event windows",
             ylim = c(-75, 75),
             formula = ~ 
             log2(e(pmax(lag(Q,-2),0.01),first)+.01) +
             log2(e(P,max)+.01) +
             e(E,mean) +
             sqrt(e(simpleSmoothTs(lag(P,-1),width=30,c=1,sides=1),first)) +
             log2(e(U,max)+.01) +
             log2(e(Q,mean)+.01),
             auto.key = list(lines = TRUE, points = FALSE, columns = 3),
             se = FALSE, pch = NA) +
    glayer(panel.rug(x = x, alpha = 0.5), groups = 1) +
    layer(grid.text(x = 1, y = 0, "(overestimated on log scale)", just = c(1.1, -2.0), gp = gpar(col = "grey30")),
          grid.text(x = 1, y = 1, "(underestimated on log scale)", just = c(1.1, 1.5), gp = gpar(col = "grey30")),
          packets = 1)
dimnames(foo)[[1]] <- 
    c("log2 ante. flow", "log2 max rain", "mean temp. (deg. C)",
      "sqrt 30-day ante. rain", "log2 max eff. rain (U)", "log2 mean flow"
      )
plot(foo)
@ 
\caption{\label{fig:resbc-each-plot} 
  Marginal effects of variables on model log-scale residual flow sums
  in \code{rain5} event windows. 
  These are GAM smooths fitted independently to the residuals
  against each covariate. All covariates are in units of mm/day
  (before transformation) except temperature which is in degrees Celcius.
}
\end{center}
\end{figure}

Building on the empirical data analysis of Figure
\ref{fig:event-runoff-gam-plot}, we can assess model residuals,
aggregated up to total flow in event windows, in relation to the same
three covariates: antecedent flow level, peak rainfall, and
temperature. If the model were explaining the data well, then there
should not be a systematic relationship with these covariates.  It is
useful to include additional covariates, to provide more information
about each event: an index of antecedent rainfall (moving average over
preceding 30 days), the peak \emph{effective} rainfall as simulated by
each model, and the mean observed flow. Figures
\ref{fig:res-each-plot} and \ref{fig:resbc-each-plot} show the
estimated relationships between model residuals and these covariates
derived from simulation over the whole Queanbeyan River time
series. Residuals were calculated as the difference between total
observed and total simulated flow in all \code{rain5} event windows
(defined in Section \ref{sec:events}).

The results show that the \emph{cmd\_raw} model does best in
accounting for antecedent conditions on the raw data scale, and even
on the log scale under wet conditions but not dry conditions. Both CMD
models provide a significant improvement over the \emph{intensity}
reference model, which does not account explicitly for antecedent
conditions. On the other hand, as might be expected, the
\emph{intensity} model does best in accounting for high intensity
rainfall (and effective rainfall), although it is still subject to an
obvious systematic effect. The \emph{cmd\_log} model is clearly
accounting for most effects better than the other models when
log-scale residuals are considered. It is still subject to systematic
effects with respect to antecedent flow, mean temperature, and mean
observed flow; all of these are likely to be correlated.

By examining and comparing systematic errors in the model residuals we
can think about how to develop improved models. For instance, there is
room for improvement in accounting for antecedent wetness. Also, we
have shown that runoff from high rainfall intensity events is being
underestimated, and that even a simple intensity model can lead to
some improvement in that respect.


\section{Conclusions and outlook}
\label{sec:conclusions}

The \pkg{hydromad} package attempts to offer the modeller a simple,
flexible and open software environment, integrated with the \R{}
system, for working with hydrological models. For rapid use it
includes a set of predefined models, calibration algorithms, and fit
statistics. We have shown how an event-based data analysis approach
can diagnose model deficiencies and guide model development. It is
useful to take a comparative approach in this regard, comparing model
performance with one or more reference models.

The package has some support for uncertainty estimation based on
parameter sampling. Markov Chain Monte Carlo methods may be used for
efficient adaptive sampling, with either a formal or an informal
likelihood function. The resulting ensemble of parameter sets may be
simulated on new data to produce point-wise confidence bounds. The
potential exists to implement the more efficient bootstrap-based
methods of \citet{SelleHannah:2010}.

As \pkg{hydromad} and \R{} are free software, it is hoped that further
methodological innovation and associated software development will
come from the wider community.


\section*{Acknowledgements}

Hak-Soo Kim discovered the interestingness of the Queanbeyan River
catchment, and generously provided the estimated areal rainfall data.


\section*{Computational details}

<<>>=
pkgVersion <- function(pkg)
    gsub("-", "--", packageDescription(pkg)[["Version"]])
@ 

The results in this paper were obtained using 
\R{}
\Sexpr{paste(R.Version()[6:7], collapse = ".")} 
with the packages
\pkg{hydromad} 
\Sexpr{pkgVersion("hydromad")}, 
\pkg{zoo} 
\Sexpr{pkgVersion("zoo")},
\citep{ZeileisGrothendieck:2005}, 
\pkg{mgcv}
\Sexpr{pkgVersion("mgcv")}
\citep{Wood:2004},
\pkg{lattice}
\Sexpr{pkgVersion("lattice")} 
\citep{lattice}, and
\pkg{latticeExtra} 
\Sexpr{pkgVersion("latticeExtra")} 
\citep{latticeExtra}.
\R{} itself and all packages used are available from
CRAN at \url{http://CRAN.R-project.org/}.

The code used to produce the results and figures in this paper is
available from \url{http://www.nfrac.org/felix/papers/}.


\section*{References}

\bibliographystyle{elsarticle-harv}
\bibliography{hydromad}

\appendix

\section{Statistics}
\label{appendix:statistics}

A set of pre-defined statistics is provided in the \pkg{hydromad}
package and are listed below. These are not intended to be a
comprehensive set, but rather for use as quick examples 
and to demonstrate how to define such functions.

Raw time-step based statistics:

\begin{itemize}
  \item \code{bias}:
    bias in data units, $\sum (X - Q)$
  \item \code{rel.bias}:
    bias as a fraction of the total observed flow,
    $\sum (X - Q) / \sum Q$
  \item \code{abs.err}:
    the mean absolute error, $\mathsf{E} |X - Q|$
  \item \code{RMSE}:
    Root Mean Squared Error, $\sqrt{\mathsf{E}(X - Q)^2}$
  \item \code{r.squared}:
    R Squared (Nash-Sutcliffe Efficiency),
    $1 - \sum (Q - X)^2 / \sum (Q - \bar{Q})^2$
  \item \code{r.sq.sqrt}:
    R Squared using square-root transformed data:
    $1 - \frac{\sum |\sqrt{Q} - \sqrt{X}|^2 }{ \sum |\sqrt{Q} - \bar{\sqrt{Q}}|^2 }$
  \item \code{r.sq.log}:
    R Squared using log transformed data, with an offset:
    $1 - \frac{\sum |\log{(Q+\epsilon)} - \log{(X+\epsilon)}|^2 }
    {\sum |\log{(Q+\epsilon)} - \bar{\log{(Q+\epsilon)}}|^2 }$.
    Here $\epsilon$ is the 10 percentile (i.e. lowest
    decile) of the non-zero values of Q.
  \item \code{r.sq.boxcox}:
    R Squared using a Box-Cox transform \citep{BoxCox:1964}. The power
    $\lambda$ is chosen to fit Q to a normal distribution. When
    $\lambda = 0$ it is a log transform; otherwise it is
    $y_* = \frac{(y+\epsilon)^\lambda - 1}{\lambda}$.
    Here $\epsilon$ is the 10 percentile (i.e. lowest
    decile) of the non-zero values of Q.
  \item \code{r.sq.diff}:
    R Squared using differences between
    successive time steps, i.e. rises and falls.
  \item \code{r.sq.smooth5}:
    R Squared using data smoothed with a triangular kernel of width 5
    time steps: \code{c(1,2,3,2,1)/9}.
  \item \code{persistence}:
    R Squared where the reference model
    predicts each time step as the previous observed value. This
    statistic therefore represents a model's performance compared to a
    naive one-time-step forecast.
  \item \code{r.sq.seasonal}:
    R Squared where the reference model
    is the mean in each calendar month, rather than the default which
    is the overall mean.
  \item \code{r.sq.vs.tf}:
    R Squared where the reference model
    is a second-order transfer function (two stores in parallel)
    fitted directly to the rainfall data, i.e. assuming a constant
    runoff ratio. The indicates the marginal improvement of including
    the SMA model component. 
  \item \code{r.sq.vs.tf.bc}:
    R Squared using a Box-Cox transform where the reference model
    is a second-order transfer function (two stores in parallel)
    fitted directly to the rainfall data, i.e. assuming a constant
    runoff ratio. The indicates the marginal improvement of including
    the SMA model component. 
    
  \item \code{X0}:
    correlation of modelled flow with the model residuals.
  \item \code{X1}:
    correlation of modelled flow with the model residuals from the previous time step.
  \item \code{ARPE}:
    Average Relative Parameter Error. Requires that a
    variance-covariance matrix was estimated during calibration.
\end{itemize}

Aggregated and event-based statistics:

\begin{itemize}
  \item \code{r.sq.monthly}:
    R Squared with data aggregated into calendar months.

  \item \code{e.rain5}:
    R Squared of flow totals in
    each event, defined by rainfall exceeding 5mm per time step, and
    continuing until the next such event. Each single event continues
    at least until rainfall remains below 1 mm for 4 time steps.
    An example of this event definition is shown in Figure X.
  \item \code{e.rain5.log}:
    same as \code{e.rain5} but the event totals are log-transformed
    (with the non-zero 10 percentile of these totals as an offset).
  \item \code{e.rain5.bc}:
    same as \code{e.rain5} but the event totals are Box-Cox-transformed
    (with the non-zero 10 percentile of these totals as an offset).

  \item \code{e.q90}:
    R Squared of flow totals in
    each event, defined by observed flow exceeding the 90 percentile
    level for at least 2 time steps, and continuing until the next
    such event. Each single event continues at least until flow falls
    below the 90 percentile level for 4 time steps, and must be
    separated from the next event by a further 5 time steps.
    An example of this event definition is shown in Figure X.
  \item \code{e.q90.log}:
    same as \code{e.q90} but the event totals are log-transformed.
  \item \code{e.q90.bc}:
    same as \code{e.q90} but the event totals are Box-Cox-transformed.
    
  \item \code{e.q90.all}:
    R Squared of flow totals in
    \emph{and between} each event, defined by observed flow exceeding
    the 90 percentile level for at least 2 time steps, and continuing
    until the flow falls below the 90 percentile level for 4 time
    steps. Events must be separated by a further 5 time steps. 
    An example of this event definition is shown in Figure X.
  \item \code{e.q90.all.log}:
    same as \code{e.q90.all} but the event totals are log-transformed.
  \item \code{e.q90.all.bc}:
    same as \code{e.q90.all} but the event totals are Box-Cox-transformed.
    
  \item \code{e.q90.min}:
    the same as \code{e.q90} but instead of flow totals in each event,
    the minimum flow in each event is extracted.
  \item \code{e.q90.min.log}:
    the same as \code{e.q90.min} but flow minima are log-transformed
    (with the non-zero 10 percentile of these minima as an offset).
  \end{itemize}


\end{document}


