\documentclass[11pt,a4paper]{article}
\pagestyle{headings}
%% link style
\usepackage{hyperref,color}
\definecolor{Red}{rgb}{0.5,0,0}
\definecolor{Blue}{rgb}{0,0,0.5}
  \hypersetup{%
    hyperindex = {true},
    colorlinks = {true},
    linktocpage = {true},
    plainpages = {false},
    linkcolor = {Blue},
    citecolor = {Blue},
    urlcolor = {Red},
    pdfstartview = {FitH},
    pdfpagemode = {UseOutlines},
    pdfview = {XYZ null null null}
  }
%% custom markup
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}
\let\code=\texttt
\let\proglang=\textsf
\def\ihacres{\textsc{ihacres}}
\def\Ihacres{\textsc{Ihacres}}
%% box the figures
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}

\title{Hydromad Tutorial}
\author{Felix Andrews}

%\VignetteIndexEntry{Getting started with hydromad}
%\VignettePackage{hydromad}

\begin{document}

\SweaveOpts{engine=R,eps=FALSE,echo=FALSE,prefix.string=figs/tutorial}

<<preliminaries, echo=FALSE, results=hide>>=
if (!file.exists("figs")) dir.create("figs")
library(hydromad)
library(xtable)
ltheme <- custom.theme.2()
ltheme$strip.background$col <- grey(7/8)
ltheme$strip.shingle$col <- grey(6/8)
ltheme$fontsize = list(text = 11)
lattice.options(default.theme = ltheme) ## set as default
ps.options(pointsize = 11)
options(width = 60, continue = " ", digits = 3)
set.seed(0)
@

\maketitle

\section{Introduction}

The \pkg{hydromad} package is designed for hydrological modelling and
associated data analysis. It is focussed on a \emph{top-down},
spatially lumped, empirical approach to environmental hydrology.  In
practice the emphasis is on models of rainfall runoff in catchments
(watersheds). Such models predict streamflow from time series of
areal rainfall and temperature or potential evapo-transpiration. They
can be calibrated to time series of observed data.

As \emph{spatially lumped} models, they do not explicitly represent
spatial variation over the catchment area. In particular, the standard
formulations do not attempt to model effects of changes in land
cover. These models are usually calibrated to a period of observed
streamflow, and the parameters defining the modelled relationship
between rainfall, evaporation and flow are assumed to be
\emph{stationary} in this period.

The modelling framework in the \pkg{hydromad} package is based on a
two-component structure: (1) a \emph{soil moisture accounting} (SMA)
module; and (2) a \emph{routing} or \emph{unit hydrograph} module
(Figure \ref{fig:model-framework}). The SMA model converts rainfall
and temperature into \emph{effective rainfall} --- the amount of
rainfall which eventually reaches the catchment outlet as streamflow
(i.e. that which is not lost as evaporation etc). The routing module
converts effective rainfall into streamflow, which amounts to defining
the peak response and shape of the recession curve. It is usually a
linear transfer function, which can be as simple as a single
exponential recession (i.e. constant decay rate), although variants
with non-linearities are also available.

\begin{figure}[hbpt!]
\begin{center}
\setkeys{Gin}{width=0.9\textwidth}
<<model-framework, fig=TRUE, width=6, height=1>>=
library(grid)
## framework diagram structure:
## --> |box| --> |box| -->
grid.newpage()
pushViewport(viewport(gp = gpar(fontsize = 10)))
arr <- arrow(length = unit(1, "char"))
## first arrows: rainfall and evaporation (inputs)
grid.lines(y = 0.75, x = c(0.0, 0.2), arrow = arr)
grid.lines(y = 0.5, x = c(0.0, 0.2), arrow = arr)
grid.lines(y = 0.25, x = c(0.0, 0.2), arrow = arr, gp = gpar(lty=2))
grid.text(y = 0.75, x = 0.1, label = "rainfall \n")
grid.text(y = 0.5, x = 0.1, label = "temp. / PET \n")
grid.text(y = 0.25, x = 0.1, label = "other inputs \n")
## first box: loss module
grid.rect(x = 0.3, width = 0.2, height = 0.9)
grid.text(x = 0.3, label = "Soil Moisture\nAccounting (SMA)\nmodel")
## second arrow: effective rainfall
grid.lines(y = 0.5, x = c(0.4, 0.6), arrow = arr)
grid.text(y = 0.5, x = 0.5, label = "effective\n rainfall")
## second box: routing module
grid.rect(x = 0.7, width = 0.2, height = 0.9)
grid.text(x = 0.7, label = "(unit hydrograph)\n routing model")
## third arrow: streamflow (output)
grid.lines(y = 0.5, x = c(0.8, 1.0), arrow = arr)
grid.text(y = 0.5, x = 0.9, label = "streamflow \n")
upViewport()
@
\caption{\label{fig:model-framework}
  The modelling framework in the \pkg{hydromad} package. 
}
\end{center}
\end{figure}

\setkeys{Gin}{width=1.0\textwidth}

The \pkg{hydromad} package is intended for:
\begin{itemize}
\item defining and fitting spatially-lumped hydrological models to
  observed data;
\item simulating these models, including model state variables and
  component flow separation.
\item evaluating and comparing these models: summarising performance
  by different measures and over time, using graphical displays
  (hydrograph, flow duration curve, residuals, etc) and statistics;
\item integration with other types of data analysis and model analysis
  in \proglang{R}, including sensitivity and uncertainty analyis.
\end{itemize}

This tutorial describes how to get started with the \pkg{hydromad}
\proglang{R} package. It covers the basics of reading data in from
files, converting it into the appropriate format, and fitting and
analysing a simple model.

Once you have \proglang{R}
running\footnote{See \url{http://www.r-project.org/}}
and have installed the \pkg{hydromad} 
package\footnote{See \url{http://hydromad.catchment.org/}}, 
you can load it:
<<load-package, echo=TRUE>>=
library(hydromad)
@


\section{Input data}

The example we will look at is the Cotter River catchment at Gingera
(gauge 410730) in the Australian Capital Territory, Australia. This is
a 148 km$^2$ catchment managed for urban water supply. Areal rainfall
was estimated from several rain gauges operated by the Bureau of
Meteorology and EcoWise. The temperature records come from Canberra
Airport.

The Cotter data is built in to the \pkg{hydromad} package, and can be
loaded into the workspace with:
<<load-Cotter-data, echo=TRUE>>=
data(Cotter)
@ 

See Appendix \ref{sec:reading-in-data} for a demonstration of reading
in the time series data from files.


\section{Data checking}

In a real data analysis problem, data checking is a central
issue. However, as this document aims to introduce the core modelling 
functions, only a simple check will be demonstrated here. The most
obvious thing to do is to plot the time series, as shown in Figure
\ref{fig:dataplot}. 

\begin{figure}[hpbt]
\begin{center}
  \emph{To plot the raw (daily) time series:}
<<rawdataplot-code, echo=TRUE, eval=FALSE>>=
xyplot(Cotter)
@
  \emph{To plot a section of the time series:}
<<rawdataplot-code, echo=TRUE, eval=FALSE>>=
xyplot(window(Cotter, start = "1974-01-01", end = "1975-01-01"))
@
  \emph{And to plot the time series aggregated to a monthly time step:}
<<dataplot-code, echo=TRUE, results=hide>>=
monthlyPQE <- aggregate(Cotter, as.yearmon, mean)
xyplot(monthlyPQE,
       screens = c("Streamflow (mm/day)", "Areal rain (mm/day)", "Temperature (deg. C)"),
       xlab = NULL)
@
<<dataplot, fig=TRUE, width=6, height=5>>=
print(trellis.last.object())
rm(monthlyPQE)
@
\caption{\label{fig:dataplot} Input data, averaged over months. }
\end{center}
\end{figure}


Table \ref{tab:datasumm} shows the mean and quartiles of each input
data series. One measure that is of key interest in hydrology is the
\emph{runoff ratio}, the proportion of the rainfall which flows out of
the catchment. In a simple case this is just \code{sum(Q) / sum(P)},
but as we have missing values, we should only compare the common
observations:

<<runoff-ratio, echo=TRUE>>=
ok <- complete.cases(Cotter[,1:2])
with(Cotter, sum(Q[ok]) / sum(P[ok]))
@ 

This figure is within the range we would expect, so is looks like we
probably have the right data series and units.

To estimate the delay time between rainfall and a consequent
streamflow response, we can look at the cross-correlation
function. The \pkg{hydromad} function \code{estimateDelay} picks out
the lag time corresponding to the maximum correlation between rainfall
and \emph{rises in} streamflow. In the Cotter this is 0 days. For more
detail there is a function \code{rollccf} which calculates the
cross-correlation in a moving window through the data, shown in Figure
\ref{fig:rollccf-plot}. When the cross-correlation value drops down
towards zero, there is little connection between rainfall and
streamflow, and you should start to worry about the data. If the lag 1
value jumps above the lag 0 value, this indicates that the delay time
has changed.

\begin{figure}[hpbt]
\begin{center}
  \emph{Rolling cross-correlation between rainfall and
    streamflow rises:}
<<rollccf-plot-code, echo=TRUE, results=hide>>=
x <- rollccf(Cotter)
xyplot(x, xlim = extendrange(as.Date(c("1980-01-01","1990-01-01"))))
@
<<rollccf-plot, fig=TRUE, height=7>>=
print(trellis.last.object())
@
\caption{\label{fig:rollccf-plot} 
  Cross-correlation between rainfall and streamflow rises, in two
  rolling windows of width 90 days and 365 days. 
}
\end{center}
\end{figure}

%See \code{vignette("dataChecking", package = "hydromad")} (\emph{under
%  construction}) for more on this topic, including analysing changes
%in the runoff ratio, cross correlation and lag times.


\section{Model Specification}

A \code{hydromad} object encapsulates the chosen model form, parameter
values (or ranges of values), as well as results. The model form is
divided into two components: SMA (Soil Moisture Accounting) and
routing. Additionally, a specification can be given for fitting the
routing component (\code{rfit}). If given, this is applied
automatically to fit the routing component after the SMA parameters
have been specified.

Let us define some data periods. We will fit a model to one, the
calibration period, and then simulate it on the other periods to
cross-check model performance.
<<define-periods, echo=TRUE>>=
ts70s <- window(Cotter, start = "1970-01-01", end = "1979-12-31")
ts80s <- window(Cotter, start = "1980-01-01", end = "1989-12-31")
ts90s <- window(Cotter, start = "1990-01-01", end = "1999-12-31")
@

When we first set up the model, most of the parameters are not
uniquely specified, but rather have a range of possible values. These
defaults are taken from \code{hydromad.options()}, and they can be
over-ridden by arguments to the \code{hydromad} function.

A nice simple starting point is the classic \ihacres{} model of
Jakeman and Hornberger (1993), which is a Soil Moisture Accounting
model referred to here as \code{"cwi"} (Catchment Wetness Index). 

The routing component typically used in \ihacres{} is a Unit
Hydrograph composed of exponential components, a structure referred to
here as \code{"expuh"}. Up to three time constants can be specified,
referred to as \code{tau\_s} (slow component $\tau_s$), \code{tau\_q} (quick
component $\tau_q$) and \code{tau\_3}. The partitioning of flow
between the stores is set by \code{v\_s} (fractional volume in the slow
component $v_s$), and by default the quick flow component is assigned
the remainder.\footnote{for more complex structures \code{v\_3} and/or
  \code{v\_q} may be specified. See the help page for details.}

When a model structure is specified, default parameter ranges for
the given SMA model are applied, and others can be specified:

<<model, echo=TRUE>>=
cotterMod <- hydromad(ts90s, sma = "cwi", routing = "expuh",
                      tau_s = c(5,100), tau_q = c(0,5), v_s = c(0,1))
print(cotterMod)
@

With this model specification, we can choose to calibrate the model in
various ways, or to simulate from the specified parameter space, or to
run sensitivity or uncertainty analysis.

\subsection{Calibration}

Currently implemented calibration methods include simple sampling
schemes (\code{fitBySampling}), general optimisation methods with
multistart or presampling (\code{fitByOptim}) and the more
sophisticated Shuffled Complex Evolution (\code{fitBySCE}) and
Differential Evolution (\code{fitByDE}) methods. All attempt to
maximise a given objective function.

The objective function can be specified as the \code{objective}
argument to these functions, or by setting
\code{hydromad.options(objective = )}.  It is given as an R function
which may refer to the values \code{Q} and \code{X}, representing
observed and modelled flow, respectively. For more advanced use it may
also refer to \code{U} (modelled effective rainfall), or the full
input \code{DATA} matrix. 

The \code{nseStat} function implements a generalisation of the
familiar $R^2$ coefficient of efficiency (Nash and Sutcliffe, 1970):

\begin{equation}
  \mathrm{nseStat}(Q,X) = \frac{ \sum |Q_* - X_*|^2 }{ \sum |Q_* - Z_*|^2 }
\end{equation}

where $Q$ and $X$ are the observed and modelled values; $Z$ is the
result from a reference model, which is the baseline for
comparison. $Z$ defaults to the mean of observed data
$\mathrm{E}(Q_*)$, corresponding to the typical $R^2$ statistic.
Subscript $*$ denotes transformed data, and the transform can be
specified. See \code{?nseStat} and \code{?hydromad.stats} for examples.

Here we use the default, which is a weighted sum of the $R^2$ of
square-root transformed data, and (with less weight) the
$R^2$ of monthly-aggregated data.

For this simple example, the model will be calibrated using the
\code{fitByOptim} function, which performs parameter sampling over the
pre-specified ranges, selecting the best of these, and then runs an
optimisation algorithm from that starting point.

<<model-fit, echo=TRUE>>=
cotterMod <- update(cotterMod, routing = "armax", rfit = list("sriv", order = c(n=2, m=1)))
cotterFit <- fitByOptim(cotterMod, samples = 100, method = "PORT")
@

See the help pages \code{help("hydromad")} and
\code{help("fitByOptim")} for details of some of the options
available.


\section{Model Output}

Now that we have an object representing a calibrated model, what can
we do with it? There are many standard \proglang{R} functions which
have methods for \code{hydromad} objects, which allow one to:

\begin{itemize}
\item \textbf{view model info} using \code{print()}, \code{summary()}, and
  \code{objFunVal()}. 
\item \textbf{extract parameter values} using \code{coef()}.
\item \textbf{access data} with \code{fitted()}, \code{residuals()}, and
  \code{observed()}. (These exclude the warm-up period by default.)
\item \textbf{run with new data} using \code{update()} or \code{predict()}.
\item \textbf{simulate from parameter ranges} using \code{simulate()}.
\item \textbf{generate plots} using \code{xyplot()}, \code{qqmath()}, etc.
\end{itemize}

For details, see the examples below, the user manual, and the help
page of each function.\footnote{Note that to get help for generic
  functions it is necessary to specify the method for \code{hydromad}
  objects: e.g. \code{?predict.hydromad} or \code{?xyplot.hydromad}.}
The help pages are also available from the web site
\url{http://hydromad.catchment.org/}.

Most basically, one can extract the modelled streamflow time series
with the function \code{fitted()}, and this can of course be used with
any of \proglang{R}'s library of analysis functions. A quick way to
view the modelled and observed streamflow time series together is to
call \code{xyplot()} on the model object, as in Figure
\ref{fig:obs-mod-plot}. Figures \ref{fig:print-hydromad} and
\ref{fig:summary-hydromad} also show the output from calling the
functions \code{print()} and \code{summary()} on the model object.

\begin{figure}[hpbt]
\begin{center}
<<obs-mod-plot-the70s-code, echo=TRUE, results=hide>>=
xyplot(cotterFit, with.P = TRUE, xlim = as.Date(c("1994-01-01", "1997-01-01")))
@
<<obs-mod-plot-the70s, fig=TRUE, height=4>>=
print(trellis.last.object())
@
\caption{\label{fig:obs-mod-plot} Observed vs modelled
  streamflow in part of the calibration period. }
\end{center}
\end{figure}


\begin{figure}[hpbt]
\begin{center}
    \emph{To display information and parameters of a model:}
<<print-model, echo=TRUE>>=
print(cotterFit)
@
\caption{\label{fig:print-hydromad}
  Printing a model to view its parameter values. Note one can get hold of
the parameter values using
\code{coef(cotterFit)} or
\code{coef(cotterFit, which = "routing")} (for the unit hydrograph only).
}
\end{center}
\end{figure}


\begin{figure}[hpbt]
\begin{center}
\emph{To display basic performance statistics for a model:}
<<summary-model-code, echo=TRUE>>=
summary(cotterFit)
@
\caption{\label{fig:summary-hydromad}
  Calculating basic performance statistics for a model. The
  \code{summary} function actually returns a list, containing the
  values of various performance statistics. }
\end{center}
\end{figure}


\section{Model Simulation}

We can simulate this model on the other periods using the \code{update} function:
<<update-newdata, echo=TRUE>>=
sim70s <- update(cotterFit, newdata = ts70s)
sim80s <- update(cotterFit, newdata = ts80s)
simAll <- update(cotterFit, newdata = Cotter)
@

For \emph{verification} purposes, we would like to calculate
performance statistics for the whole dataset but excluding the
calibration period. The easiest way to do this is to set the observed
streamflow data in the calibration period to \code{NA} (missing), and
then run the simulation:
<<verfication-period-one, echo=TRUE>>=
tsVerif <- Cotter
tsVerif$Q[time(ts90s)] <- NA
simVerif <- update(cotterFit, newdata = tsVerif)
@

It is convenient to group these models together into a \code{runlist},
which is just a list of fitted models:
<<runlist, echo=TRUE>>=
allMods <- runlist(calibration = cotterFit, sim70s, sim80s, simVerif)
@

The predicted time series (hydrograph) and cumulative distribution
(flow duration curve) can be generated as in Figures
\ref{fig:obs-mod-plots} and \ref{fig:fdc-plot}.

\begin{figure}[hpbt]
\begin{center}
<<obs-mod-plots-code, echo=TRUE, results=hide>>=
xyplot(allMods[2:3], scales = list(y = list(log = TRUE)))
@
<<obs-mod-plots, fig=TRUE>>=
print(trellis.last.object())
@
\caption{\label{fig:obs-mod-plots} Observed vs modelled
  streamflow in validation periods. }
\end{center}
\end{figure}


\begin{table}[hpbt]
\begin{center}
<<mod-cal-stats-table-code, echo=TRUE, eval=FALSE>>=
summary(allMods)
@
<<mod-cal-stats-table, results=tex>>=
perfstats <- summary(allMods)
print(xtable(perfstats), floating = FALSE)
@
\caption{\label{tab:mod-cal-stats} Performance statistics for a set of models. }
\end{center}
\end{table}


\begin{figure}[hpbt]
\begin{center}
<<mod-1990s-summary-table-code, echo=TRUE>>=
summary(simAll, breaks = "5 years")
@
\caption{Viewing a break-down the performance of a model over 5-year blocks. }
\end{center}
\end{figure}


\begin{figure}[hpbt]
\begin{center}
  \emph{To plot performance statistics over time:}
<<r2-breaks-plot-code, echo=TRUE, results=hide>>=
twoYrStats <- summary(simAll, breaks = "2 years")
statSeries <- twoYrStats[,c("r.squared", "r.sq.sqrt", "rel.bias", "runoff")]
## cut off crazy R Squared values below 0 (for plotting)
statSeries[,1:2] <- pmax(statSeries[,1:2], 0)
c(xyplot(statSeries, type = "s", lwd = 2,
         ylab = "statistic", xlab = NULL),
  `observed streamflow` = xyplot(observed(simAll)),
  layout = c(1,5), x.same = TRUE) +
    layer_(panel.refline(h = 0, v = time(statSeries)))
@
<<r2-breaks-plot, fig=TRUE, height=7>>=
print(trellis.last.object())
@
\caption{\label{fig:r2-breaks-plot} Performance statistics
  plotted over time in regular 2 year blocks. The runoff coefficient and
  observed streamflow data are also shown. }
\end{center}
\end{figure}


\begin{figure}[hpbt]
\begin{center}
  \emph{To plot the flow duration curve for modelled vs observed
    data in the calibration period:}
<<fdc-1-plot-code, echo=TRUE, eval=FALSE>>=
qqmath(cotterFit, scales = list(y = list(log = TRUE)), type = c("l","g"))
@
  \emph{To plot a flow duration curve for each of the simulated models:}
<<fdc-plot-code, echo=TRUE, results=hide>>=
qqmath(allMods, type = c("l","g"), 
       scales = list(y = list(log = TRUE)),
       xlab = "Standard normal variate",
       ylab = "Flow (mm/day)", 
       f.value = ppoints(100), tails.n = 50,
       as.table = TRUE)
@
<<fdc-plot, fig=TRUE, height=7>>=
print(trellis.last.object())
@
\caption{\label{fig:fdc-plot} Log-normal Daily Flow Duration Curve for models in
  simulation. }
\end{center}
\end{figure}


\section{Model and Calibration Options}

There are several extensions to the basic model used so far. With
different types of data, such as very dry or wet catchments, sub-daily
time steps, poor areal rainfall estimates, cases of baseflow loss to
groundwater, etc, different models or calibration methods will need to
be used.


\subsection{Model Structure and Parameter Ranges}

We have used an \ihacres{} CWI model in this tutorial, which is a
simple metric type model. Other SMA models are included in the
package, or one can define a new model. See the user manual for details.

Ranges of parameters to search when calibrating the effective rainfall
model can be specified as arguments to the \code{hydromad} or
\code{update()} functions. Alternatively, parameters can be fixed to a
given value by specifying a single number.

The default ranges can be seen, and set, using the function
\code{hydromad.options()}. 

The example, in the CWI model, the threshold parameter \code{l} (used
for intermittent or ephemeral rivers), defaults to a fixed value of
0. To allow calibration of this parameter, specify a numerical
range. Similarly, the evapo-transpiration coefficient \code{e}
defaults to the range $[0.01,1.5]$; to fix it to a given value, just
specify it:

<<echo=TRUE, eval=FALSE>>=
hydromad(ts90s, sma = "cwi", l = c(0, 200), e = 0.166)
@

\subsection{Optimisation settings}

Each of the fitting functions has several options, and the help pages
should be consulted for details. An important option is the choice of
objective function; see the discussion above about how to specify it.

In the simple cases of using \code{fitBySampling} or
\code{fitByOptim}, the argument \code{samples} specifies how many
random parameter sets will be sampled (from the predefined parameter
ranges), and argument \code{sampletype} chooses Uniform Random, Latin
Hypercube, or ``all combinations'' (a regular grid of values). The one
model with best objective function value is chosen. In the case of
\code{fitByOptim} this is then improved locally with an optimisation
algorithm.


\subsection{Unit Hydrograph Transfer Functions}

A typical unit hydrograph model, at least in \ihacres{} models, is a
linear \emph{transfer function}, i.e. an ARMAX-like (Autoregressive,
Moving Average, with eXogenous inputs). This can often, but not
always, be formulated mathematically as a set of exponentially receding
stores, which may be in a parallel and/or series
configuration. ARMAX-type models are specified by the number of
auto-regressive terms $n$ and the number of moving average terms
$m$. For example, a model with one store is $(n=1,m=0)$; two stores in
parallel is $(n=2, m=1)$; two stores and an instantaneous store in parallel
is $(n=2, m=2)$. Three stores in parallel is $(n=3,m=2)$.

When using \code{armax} or \code{expuh} routing, specialised methods
are available to estimate for calibration, such as the \code{SRIV} (Simple
Refined Instrumental Variable) algorithm. These are specified using
the \code{rfit} argument. 

The order of the transfer function may be varied, as well as the delay
time. If there is any ambiguity in choosing the best delay time, each
possibility should be tried. 

To test different model structures systematically, a convenience
function \code{tryModelOrders} is provided. An example is given in
Table \ref{tab:try-model-orders}. In this case a simple 
SMA is used with fixed parameters.

For more information on these issues see, for example,
Jakeman et. al. (1990) and Young (2003).

\begin{table}[hpbt]
\begin{center}
<<try-model-orders-table-code, echo=TRUE, results=hide>>=
ihSpec <- hydromad(ts90s, sma = "cwi", tw = 10, f = 1, 
                   routing = "armax")
osumm <- tryModelOrders(update(ihSpec, rfit = "sriv"),
                        n = 0:3, m = 0:3, delay = 0)
summary(osumm)
@
<<try-model-orders-table, results=tex>>=
perfstats <- summary(osumm)
print(xtable(perfstats, digits = 3), floating = FALSE)
@
\caption{\label{tab:try-model-orders}
  Fit and information statistics from fitting different unit
  hydrograph transfer functions with SRIV algorithm. 
  ARPE is the Average Relative Parameter Error estimated by SRIV. The
  effective rainfall input was generated by an \ihacres{} CWI model
  with fixed parameters. } 
\end{center}
\end{table}


\subsection{Unit Hydrograph Inverse Fitting Methods}

Unit Hydrograph routing models are typically fitted using least
squares or SRIV algorithms, but this depends on the modelled effective
rainfall, and so must be continuously re-fitted while calibrating the
SMA model. One alternative is to fit the unit hydrograph to the
observed streamflow data directly -- though usually
constrained by rainfall -- and then use that as a fixed component
while calibrating the SMA model. This can be done using an inverse
filtering method with
\code{rfit = list("inverse", ...)}. (There are many options here also).

Other such inverse fitting methods are possible, e.g. average event
unit hydrograph estimation, but are not currently implemented in this
package. 


\subsection{Other Options}

If model calibration is failing, you can set
\code{hydromad.options(trace = TRUE)} and/or
\code{hydromad.options(catch.errors = FALSE)} to track down what is
happening.

It is sometimes useful to look at the model state variables, available
as \code{predict(mod, return\_state = TRUE)} (for the SMA model), or
\code{predict(mod, return\_components = TRUE)} (for the routing
model), to see if they look sensible.

Some other things to try are
\begin{itemize}
  \item using different calibration periods;
  \item changing the warmup period length;
  \item changing the optimisation method and/or settings.
\end{itemize}


\section{What Next?}

This document has described only a basic model fitting process.

%An overview of the available models and options is given in the user
%manual, which can be accessed as \code{vignette("hydromad")}.

Help pages are available for most functions, and these are also
available online at \url{http://hydromad.catchment.org/}. There is also
a set of demos: see \code{demo(package = "hydromad")} for a list.

Please discuss any problems or suggestions with the package maintainer.


\appendix

\section{Reading in data}
\label{sec:reading-in-data}

The required input data files for this tutorial are included with the
\pkg{hydromad} package, in the \code{doc} directory. Note that the
processed data is available directly in \proglang{R} -- just type
\code{data(Cotter)} -- but this section shows how to read it in from
text files as an exercise.

A few simple steps are required to import and convert the data into a
usable form: extracting dates from the files, converting streamflow
from ML/day to mm/day, handling missing data values, and aligning the
three time series in a common time period.

Let's first view the content of one of the input files.
Set the working directory to where the data file is:
<<view-files, echo=TRUE, eval=FALSE>>=
setwd(system.file("doc", package = "hydromad"))
file.show("pq_cotter.csv")
@
<<view-files-script, echo=FALSE>>=
cat(readLines("pq_cotter.csv", n = 5), "...", sep = "\n")
@

There is no header in the file, but we know that the columns represent
rainfall (P), streamflow (Q) and date of observation. The temperature
file is similar. Knowing the format and columns we can use
\code{read.table} to import the data:
<<read-files, echo=TRUE>>=
## rain and flow data
pqdat <- read.table("pq_cotter.csv", sep = ",",
                    col.names = c("P", "Q", "Date"), as.is = TRUE)
## temperature data
tdat <- read.table("t_cotter.csv", sep = ",",
                   col.names = c("T", "Date"), as.is = TRUE)
@
and view the structure of the resulting data frames:
<<view-str, echo=TRUE>>=
str(pqdat)
str(tdat)
@

So far, the \code{Date} columns are only text; \proglang{R} does not
know they are dates. We need to specify the date format, where
\code{\%d} is day, \code{\%m} is month number, \code{\%b} is month
name, \code{\%Y} is four-digit year and \code{\%y} is two-digit year
(see \code{?strptime}).
<<convert-dates, echo=TRUE>>=
pqdat$Date <- as.Date(pqdat$Date, "%d/%m/%Y")
tdat$Date <- as.Date(tdat$Date, "%d/%m/%Y")
@

If the day, month and year were in separate columns of the file, with
names \code{"day"}, \code{"mon"} and \code{"yr"} then you could do
something like:
<<convert-dates-from-columns, echo=TRUE, eval=FALSE>>=
pqdat$Date <- with(pqdat, as.Date(ISOdate(yr, mon, day)))
@

If you have sub-daily time steps, a good choice is to use the
\code{chron()} function from the \pkg{chron} package to represent the
time index.\footnote{There is a time class built into \proglang{R},
  called \code{POSIXct}, but this represents time zones which can
  sometimes lead to complications.}

Negative values (-99) in the \emph{pq} input file represent missing
data; in \proglang{R} they should be set to the special value
\code{NA}.  Also, some dates in the temperature file are blank, and
need to be removed.
<<missing-values, echo=TRUE>>=
pqdat$P[pqdat$P < 0] <- NA
pqdat$Q[pqdat$Q < 0] <- NA
tdat <- subset(tdat, !is.na(Date))
@

The \pkg{hydromad} model fitting functions require that rainfall and
streamflow are given in the same units, typically mm / day. The
streamflow data in our input file is measured in ML / day, so we need
to convert it, supplying the catchment area of 148 km$^2$.
<<convert-to-mm, echo=TRUE>>=
pqdat$Q <- convertFlow(pqdat$Q, from = "ML", area.km2 = 148)
@

For simple applications, when the data series are already
synchronised, this data frame (or matrix) format may be enough.
However, there are benefits in working with actual \emph{time series}
objects: because they handle observation times, they allow powerful
merging, treatment of missing values, rolling averages and other
functions. While \proglang{R} has a built-in structure for regular
time series (\code{ts}), these do not handle specific dates or times,
only index numbers. It is recommended to work with \code{zoo} objects
(using the \pkg{zoo} package).\footnote{\pkg{zoo} objects are a
  generalisation of \code{ts} objects and in many cases can be used in
  the same way; see \code{vignette("zoo")}.}

<<zoo-objects, echo=TRUE>>=
library(zoo)
tsPQ <- zoo(pqdat[,1:2], pqdat$Date, frequency = 1)
tsT <- zoo(tdat[,1], tdat$Date, frequency = 1)
@

We can now merge the time series together into a final dataset.  Note
that the \pkg{hydromad} package expects temperature or
evapo-transpiration data to be called \code{E}, not
\code{T}.\footnote{This avoids name conflicts since in \proglang{R},
  \code{T} is a shorthand for \code{TRUE}.}
<<zoo-merge, echo=TRUE>>=
Cotter <- merge(tsPQ, E = tsT, all = FALSE)
@

Print the first few rows (the \emph{head}) of the time series, to
check that everything looks OK:

<<zoo-head, echo=TRUE>>=
head(Cotter, 6)
range(time(Cotter))
@

This shows that the rainfall data has missing values at the
beginning. At the other end of the series, Streamflow data is missing.
This will not cause a problem, but let us tidy it up anyway:

<<zoo-na-trim, echo=TRUE>>=
Cotter <- na.trim(Cotter)
@

The final dataset extends from \Sexpr{start(Cotter)} to
\Sexpr{end(Cotter)}, and is shown in Figure \ref{fig:dataplot} and
Table \ref{tab:datasumm}:

<<datasummary-code, echo=TRUE, eval=FALSE>>=
summary(Cotter)
@

<<datasummary, results=tex>>=
summ <- numericSummary(Cotter)
xtable(summ, caption="Data summary.
P = precipitation (mm/day), E = temperature (deg. C), Q = streamflow (mm/day).",
	label="tab:datasumm")
@



\section*{Computational details}

The results in this paper were obtained using \proglang{R}
\Sexpr{paste(R.Version()[6:7], collapse = ".")} with the packages
\pkg{hydromad} \Sexpr{gsub("-", "--", packageDescription("hydromad")$Version)},
\pkg{zoo} \Sexpr{gsub("-", "--", packageDescription("zoo")$Version)} and
\pkg{latticeExtra} \Sexpr{gsub("-", "--", packageDescription("latticeExtra")$Version)}.
\proglang{R} itself and all packages used are (or will be) available from
CRAN at \code{http://CRAN.R-project.org/}.


\end{document}
